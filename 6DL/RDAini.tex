\section{Normalized Initialization}
In order to find a good initialization for deep networks, Kaiming He(2015) followed Xavier(2010) to study the propagations of signals from input layer to output layer.
\paragraph{Forward propagation}
The central idea is to investigate the variance of the responses in each layer. For a conv layer, a response is:
$$
\mathbf{y}_{l}=\mathrm{W}_{l} \mathbf{x}_{l}+\mathbf{b}_{l}
$$
Here, the number of input channels is c, the number of output channel is d, and the size of filter is $k\times k$. Then $n=k^2c$ denoting the number of connections of a response. Using l to index a layer, and $ \mathbf{x}_{l} = f( \mathbf{y}_{l-1})$ where f is the activation.

Let initialized elements in $\mathrm{W}_{l}$ be i.i.d. Assuming that the elements in $\mathrm{x}_{l}$ are also i.i.d and $\mathrm{W}_{l}$ and $ \mathrm{W}_{l}$ are independent of each other.Then we have:
$$
\operatorname{Var}\left[y_{l}\right]=n_{l} \operatorname{Var}\left[w_{l} x_{l}\right]
$$
where now $y_{l}, x_{l},$ and $w_{l}$ represent the random variables of
each element in $y_{l}, W_{l},$ and $x_{l}$ respectively. We let $w_{l}$ have zero mean, then we have:
$$
\operatorname{Var}\left[y_{l}\right]=n_{l} \operatorname{Var}\left[w_{l}\right] E\left[x_{l}^{2}\right]
$$
If we let $w_{l-1}$ have a symmetric distribution around zero
and $b_{l-1}=0,$ then $y_{l-1}$ has zero mean and has a symmetric
distribution around zero. This leads to $E\left[x_{l}^{2}\right]=\frac{1}{2} \operatorname{Var}\left[y_{l-1}\right]$
when $f$ is ReLU. We obtain:
\begin{equation}
\operatorname{Var}\left[y_{l}\right]=\frac{1}{2} n_{l} \operatorname{Var}\left[w_{l}\right] \operatorname{Var}\left[y_{l-1}\right]
\end{equation}
With $L$ layers put together, we have:
$$
\operatorname{Var}\left[y_{L}\right]=\operatorname{Var}\left[y_{1}\right]\left(\prod_{l=2}^{L} \frac{1}{2} n_{l} \operatorname{Var}\left[w_{l}\right]\right)
$$
A proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially.A sufficient condition is:
\begin{equation}\label{He}
\frac{1}{2} n_{l} \operatorname{Var}\left[w_{l}\right]=1, \quad \forall l
\end{equation}
This leads to a zero-mean Gaussian distribution whose standard deviation (std) is $\sqrt{2 / n_{l}}$ . This is their way of initialization. they also initialize $\mathbf{b}=0$.

The mainly difference between He and Xavier is that Xavier  is based on the assumption that the activations are linear, which leads the condition in Xavier is:
\begin{equation}\label{Xavier}
 n_{l} \operatorname{Var}\left[w_{l}\right]=1, \quad \forall l
\end{equation}
And another difference is that Xavier use the uniform distribution but He use the Gaussian distribution.
\subsection{Experiments on ImageNet}
Given a 22-layer model, in cifar10 the convergence with He's initialization is faster than Xavier, but both of them are able to converge and the validation accuracies with two different initialization are about the same(error is 33.82,33.90).

With extremely deep model with up to 30 layers, He's initialization is able to make the model convergence. On the contrary, the "Xavier" method completely stalls the learning.


\section{Initialization In RDA}\label{sec_ini}
%In the original paper \cite{xiao2010dual}, RDA requires the loss function to be convex, and $w_1=0$ as an initialization. 
%In general, CNN is not convex, neither is zero initialization of $w_1$ suitable since in this case most parameters' gradients will become zero, which makes the model nearly not trainable with Prox-SGD or RDA. Thus, non-zero initialization is a necessary condition in Algorithm \ref{RDA}, we then prove in Theorem \ref{Convergence_Modified_RDA} that the convergence rate for convex problem based on this modification is still $O(\frac{1}{\sqrt t})$ when $\xi_t=O(\sqrt{t})$.  
In the original paper \cite{xiao2010dual}, RDA requires $w_1=0$ as an initialization. 
In general, zero initialization for $w_1$ is not suitable, since in this case most weights' gradients will become zero, which makes the model almost not trainable with Prox-SGD or RDA. As a result, non-zero initialization is a necessary condition in RDA for CNN. This modification will not influence the convergence of the algorithm. As proven in Theorem \ref{Convergence_Modified_RDA}, the convergence rate for convex problems based on this modification is still $O(\frac{1}{\sqrt t})$ when $\beta_t=O(\sqrt{t})$.  
\begin{theorem}\label{Convergence_Modified_RDA}
	Assume the loss function $f(w,z)$ in the problem (\ref{regularized opt problem}) is convex and there exists an optimal solution $w^{\star}$ to the problem (\ref{regularized opt problem}) with $\Psi(w)=\lambda\|w\|_1$ that satisfies $\frac{1}{2}\|w^{\star}\|_{2}^2\leq D^2$ for some $D>0$. Let the sequences $\{w_t\}_{t\geq 1}$ be generated by Algorithm \ref{RDA}, and assume $\|g_t\|_{\ast}\leq G$ for some constant $G$. Then the expected cost $\mathbf{E}\phi(\bar{w}_t)$ converges to $\phi^{\star}$ with rate $O(\frac{1}{\sqrt{t}})$
	$$
	\mathbf{E}\phi(\bar{w}_t)-\phi^{\star}= O(\frac{1}{\sqrt{t}}),
	$$
\end{theorem}
with $\bar w_t=\frac{1}{t} \sum_{\tau=1}^{t} w_\tau$ and $\phi^{\star}=\phi(w^{\star})$. \\
%See Appendix A for the proof.
%\input{Appendix.tex}
In particular, when the activation function is ReLU, the weights in CNN are usually initialized with a uniform or a normal distribution \cite{lecun2012efficient,glorot2010understanding,he2015delving}. For RDA, we propose to initialize the weights with a uniform distribution $\mathcal{U}(-b,b)$, where 
\begin{equation}
b=\sqrt{\frac{s}{n}}.
\end{equation}
For a convolutional layer, $n=k^2 c$ is the size of the filter, where $c$ is the number of input channels and $k$ is the width of the filter. For a fully connected layer, $n$ is the dimension of the input vector. In both cases, $s$ is a scalar to increase the weights (e.g. \cite{he2015delving} proposes to choose $s=6$). 

\begin{table}[tb]
	\caption{Different initialization scalars on ResNet-18, CIFAR-10 with RDA. This table shows TOP-1 and TOP-5 accuracy on validation dataset. All models are trained for 120 epochs.}
	\label{table:ini_CIFAR10}
	\vskip 0.15in
	\begin{center}
		\begin{small}
%			\begin{sc}
				\begin{tabular}{lccccr}
					\hline
					$\sqrt{s}$ &  TOP-1 &  TOP-5 &  Sparsity\\
					\hline
					1, 2    &  10.00 &  50.00 &  N/A      \\
					3      &  85.52 &  99.24 &  0.98    \\
					4      &  86.72 &  99.45 &  0.97     \\
					5      &  90.03 &  99.44 &  0.95     \\
					%6      &  90.36 &  99.55 &  0.94     \\
					%7      &  90.07 &  99.55 &  0.91     \\
					10     &  90.67 &  89.50 &  0.94    \\
					%20     &  89.14 &  99.50 &  0.94    \\
					%50     &  91.42 &  99.66 &  0.71  \\
					100    &  \bf 91.41 &  99.58 &  0.84 \\
					%200    &  90.57 &  99.56 &  0.73 \\
					1000   &  90.36 &  99.62 &  0.63 \\
					10000  &  71.80 &  97.94 &  0.34 \\
					20000  &  68.06 &  97.39 &  0.99 \\
					\hline
				\end{tabular}
%			\end{sc}
		\end{small}
	\end{center}		
\end{table}

\begin{table}[tb]
	\caption{Different initialization scalars on ResNet-18, CIFAR-100 with RDA. 
		%		$\lambda=10^{-8}$ and $\gamma=0.1$. 
		This table shows TOP-1 and TOP-5 accuracy on validation dataset. All models are trained for 120 epochs.}
	\label{table:ini_CIFAR100}
	\vskip 0.15in
	\begin{center}
		\begin{small}
%			\begin{sc}
				\begin{tabular}{lccccr}
					\hline
					$\sqrt{s}$ &  TOP-1 & TOP-5  &  Sparsity \\ 
					\hline
					1      &  63.67 &  87.85 &  0.91 \\
					2      &  \bf66.90 &  88.53 & 0.60  \\
					5      &  65.47 &  88.09 &  0.60 \\
					10     &  65.54 &  88.21 &  0.42 \\
					15     &  64.22 &  87.53 &  0.43 \\
					%			20     &  67.78 &  89.65 &  0.50 \\
					25     &  63.06 &  88.10 &  0.50 \\
					30     &  62.75 &  86.80 &  0.42 \\
					50     &  64.48 &  87.14 &  0.38   \\
					100    &  60.00 &  86.14 &  0.36  \\
					\hline
				\end{tabular}
%			\end{sc}
		\end{small}
	\end{center}
\end{table}

Since $f$ is non-linear, the effect of initialization on $g_1$, the gradient of $w_1$, is not that clear. Assuming that $f$ is a linear function, then $g_1$ is scaled in the same way as $w_1$. Since with a thresholding (ignoring the initial learning rate $\eta_1=1$), $g_1$ becomes the value of $w_2$, the initial value should not be too small, nor should it be too large because of the exploding gradient problem \cite{pascanu2012understanding}, as shown in Table \ref{table:ini_CIFAR10} and Table \ref{table:ini_CIFAR100}. We listed some good choices for $s$ in Table \ref{table:goods}.


\begin{table}[tb]
	\caption{Suitable $\sqrt{s}$ for different models and datasets. ImageNet represents ILSVRC2012.
		%		\blue{add the corresponded results, accuracy, sparsity? Can we make some summary for this choice of $s$?}
	}
	\label{table:goods}
	\vskip 0.15in
	\begin{center}
		\begin{small}
%			\begin{sc}
				\begin{tabular}{lcr}
					\hline
					Dataset & Model & $\sqrt{s}$ \\
					%					$\sqrt{s}$ & ResNet-18 & VGG-16bn  &  VGG-19bn \\
					\hline
					%					CIFAR-10 &  10 & 20 & 10 \\
					%					CIFAR-100 &  2 &  & 40 \\
					%					ILSVRC2012 & 2 & &  \\
					%					Model & Dataset & $\sqrt{s}$ \\
					CIFAR-10 & ResNet-18 & 10  \\
					& VGG-16bn & 20 \\
					& VGG-19bn & 10 \\
					CIFAR-100& ResNet-18 & 2  \\
					& VGG-16bn & 60 \\
					& VGG-19bn & 40 \\
					ImageNet & ResNet-18 & 2 \\
					%					& VGG-16bn &  \\
					%					& VGG-19bn & \\
					\hline
				\end{tabular}
%			\end{sc}
		\end{small}
	\end{center}		
\end{table}