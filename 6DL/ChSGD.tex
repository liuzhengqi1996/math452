\section{Stochastic gradient descent method and convergence theory}
The next optimization problem is the most common case in machine learning.
\begin{problem}
	\begin{equation}\label{SGDproblem}	
 	\min_{x \in \mathbb{R}^n} f(x)\quad \mbox{and}\quad f(x) = \frac{1}{N} \sum_{i=1}^N f_i(x).
	\end{equation}
\end{problem}
One version of stochastic gradient descent (SGD) algorithm is:
\begin{algorithm}\caption{SGD}
\label{alg:SGD}
{\bf Input}: initialization $x_0$, learning rate $\eta_t$.

{\bf For}: t = 0,1,2,$\dots$ 

Randomly pick $i_t \in \{1, 2, \cdots, N\}$ independently with probability $\frac{1}{N}$
\begin{equation}\label{equ:sgd-iteration}
x_{t+1} = x_{t} - \eta_t \nabla f_{i_t}(x_t).
\end{equation}
\end{algorithm}


\subsection{Convergence of SGD}
\begin{theorem}Assume that each $f_i(x)$ is $\lambda$-strongly convex and $\|\nabla f_i(x)\| \le M$ for some $M >0$.
	If we take $\eta_t = \frac{a}{\lambda (t+1)}$ with sufficiently large $a$ such that 
	\begin{equation}\label{key}
	\|x_0 - x^*\|^2 \le \frac{a^2M^2}{(a-1)\lambda^2}
	\end{equation}
then 
\begin{equation}
\mathbb{E}e_{t}^2 \le \frac{a^2M^2}{(a-1)\lambda^2 (t+1)}, \quad  t\ge 1,
\end{equation}
where $e_t = \|x_t - x^*\|$.
\end{theorem}

\begin{proof}
The $L^2$ error of SGD can be written as
\begin{equation}
      \label{equ:L2SGD}
      \begin{split}
            \mathbb{E} \|x_{t+1} - x^*\|^2 &\le \mathbb{E}\| x_{t} - \eta_t \nabla f_{i_t}(x_t) - x^* \|^2 \\
            &\le \mathbb{E} \|x_t - x^*\|^2 
            - 2 \eta_t \mathbb{E} (\nabla f_{i_t}(x_t) \cdot (x_t - x^*)) 
            + \eta_t^2 \mathbb{E} \|\nabla f_{i_t}(x_t)\|^2 \\
            & \le \mathbb{E} \|x_t - x^*\|^2 - 2 \eta_t \mathbb{E} (\nabla f (x_t) \cdot (x_t - x^*))
            + \eta_t^2 M^2 \\
            & \le \mathbb{E} \|x_t - x^*\|^2 -  \eta_t \lambda \mathbb{E} \|x_t - x^*\|^2 + \eta_t^2 M^2 \\
            & = (1 - \eta_t\lambda) \mathbb{E} \|x_t - x^*\|^2 + \eta_t^2 M^2
      \end{split}
\end{equation}
The third line comes from the fact that
\begin{equation}\label{key}
\begin{aligned}
\mathbb{E} (\nabla f_{i_t}(x_t) \cdot (x_t - x^*))  &= \mathbb{E}_{i_1i_2\cdots i_t} (\nabla f_{i_t}(x_t) \cdot (x_t - x^*)) \\
&= \mathbb{E}_{i_1i_2\cdots i_{t-1}} \frac{1}{N} \sum_{i=1}^N \nabla f_i(x_t)\cdot (x_t - x^*) \\
&= \mathbb{E}_{i_1i_2\cdots i_{t-1}}  \nabla f(x_t)\cdot (x_t - x^*) \\
&= \mathbb{E}\nabla f(x_t)\cdot (x_t - x^*),
\end{aligned}
\end{equation}
and
\begin{equation}
\mathbb{E} \|\nabla f_{i_t}(x_t)\|^2 \le \mathbb{E} M^2 = M^2.
\end{equation}
Note when $t=0$, we have 
\begin{equation}\label{key}
\mathbb{E} e_0^2 = \|x_0 - x^*\|^2 \le \frac{a^2M^2}{(a-1)\lambda},
\end{equation}
based on the assumption.

In the case of SDG, by the inductive hypothesis, 
\begin{equation}
      \begin{split}
            \mathbb{E}e_{t+1}^2 & \le (1 - \eta_t\lambda)\mathbb{E}e_{t}^2  + \eta_t^2 M^2\\
            &\le  (1 - \frac{a}{t+1}) \frac{a^2M^2}{(a-1)\lambda^2 (t+1)} + \frac{a^2M^2}{\lambda^2 (t+1)^2} \\
            & \le \frac{a^2M^2}{(a-1)\lambda^2} \frac{1}{(t+1)^2}(t+1 -a + a-1) \\
            & = \frac{a^2M^2}{(a-1)\lambda^2} \frac{t}{(t+1)^2} \\
            & \le \frac{a^2M^2}{(a-1)\lambda^2(t+2)}. \quad \left(\frac{t}{(t+1)^2} \le \frac{1}{t+2}\right),
      \end{split}
\end{equation}
which completes the proof.
\end{proof}


\subsection{SGD with mini-batch}
Firstly, we will introduce a natural extended version of the SGD discussed above with introducing mini-batch. 

\begin{algorithm}[H]\caption{SGD with mini-batch}
	\label{alg:SGD}
	{\bf Input}: initialization $x_0$, learning rate $\eta_t$.
	
	{\bf For}: t = 0,1,2,$\dots$ 
	
	\begin{center}
	Randomly pick $B_t \subset \{1, 2, \cdots, N\}$ independently 
	with probability $\frac{m!(N-m)!}{N!}$ \\
	and $\# B_t = m$.% with $\frac{1}{\tbinom{N}{m}} = $.
	\end{center}
	\begin{equation}\label{equ:sgd-iteration}
	x_{t+1} = x_{t} - \eta_t g_t(x_t).
	\end{equation}
	\quad where 
	$$
	g_{t}(x_t) = \frac{1}{m} \sum_{i \in B_{t}}  \nabla f_i(x_t)
	$$
\end{algorithm}

Now we introduce the SGD algorithm with mini-batch without replacement which is the most commonly used version of SGD in machine learning.
\begin{algorithm}[H]
	\caption{Shuffle SGD with mini-batch}
	\label{alg:mini-batch}
	{\bf Input}: learning rate $\eta_k$, mini-batch size $m$, parameter initialization $ x_{0}$ and denote $M = \lceil \frac{N}{m} \rceil$. 
	
	{\bf For} Epoch $k = 1,2,\dots$
	
	\begin{center}
	Randomly pick $B_t \subset \{1, 2, \cdots, N \}$ without replacement \\
	with $\# B_t = m$ for $t = 1,2,\cdots,M$.
	\end{center}
	
	\quad{\bf For} mini-batch $t = 1:M$
	
	\quad Compute the gradient on $B_{t}$:
	$$
	g_{t}(x) = \frac{1}{m} \sum_{i \in B_{t}}  \nabla f_i(x)
	$$
	\quad Update $x$:
	\begin{equation*}
	x  \leftarrow  x - \eta_k g_t(x),
	\end{equation*} 
	\quad {\bf EndFor}


	{\bf EndFor}
\end{algorithm}

To "randomly pick $B_i \subset \{1, 2, \cdots, N \}$ without replacement
with $\# B_i = m$ for $i = 1,2,\cdots,t$'', we usually just randomly shuffle the index set first and
then consecutively pick every $m$ elements in the shuffled index set. That is the reason why we  
would like to call the algorithm as shuffled SGD while this is the mostly used version of SGD in machine learning.

\begin{remark}	
Let us recall a general machine learning loss function
\begin{equation}\label{key}
L(\theta) = \frac{1}{N}\sum_{i=1}^N \ell(h(X_i; \theta), Y_i),
\end{equation}
where $\{(X_i, Y_i)\}_{i=1}^N$ correspond to these data pairs. 
For example, $\ell(\cdot, \cdot)$ takes cross-entropy and $h(x; \theta) = \bm p(x;\theta)$ as we discussed in Section \ref{sec:LR}.  
Thus, we have the following corresponding relation
\begin{equation*} 
f(x) \leftrightsquigarrow L(\theta), \quad
f_i(x) \leftrightsquigarrow \ell(h(X_i; \theta), Y_i). 
\end{equation*}
\end{remark}



\endinput
Here we can take the expectation of $i_t$ independently from $\{x_i, i = 1,\ldots,t\}$
to obtain the second line of (\ref{equ:L2SGD}) since $i_t$ is independent from 
$\{x_i, i = 1,\ldots,t\}$ which is completely determined by $\{i_j, j = 1,\ldots,t - 1\}$.
And the third line of (\ref{equ:L2SGD}) is obtained from the boundness of the gradients.

Here shuffle  index and get mini-batch $B_1, \cdots, B_{\frac{N}{m}}$ means that $\# B_i = m$ and 
\begin{equation}\label{key}
\{1, 2, \cdots, N\} = \cup_{i = 1}^{\frac{N}{m}} B_i,  \quad B_i \cap B_j = \emptyset
\end{equation}
where $B_i$ is chosen from $\{1, 2, \cdots, N\}$ randomly.

