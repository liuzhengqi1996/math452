
Of course, we can use this to prove a high probability result.
\begin{corollary}
 Under the assumptions of the preceding lemma, we have
 \begin{equation}
\mathbb{\bar P}\left [(\mathbb{E}g-\frac1n\sum_{i=1}^n
    g(\omega_i))^2 >\epsilon\right ] 
\le  \frac{1}{n\epsilon}\|g\|^2_{L^\infty}
 \end{equation}
\end{corollary}
\begin{proof}
 \begin{equation}
  \mathbb{\bar P}\left [(\mathbb{E}g-\frac1n\sum_{i=1}^n
    g(\omega_i))^2 >\epsilon\right ] 
\le \epsilon^{-1}
    \mathbb{\bar E}(\mathbb{E}g-\frac1n\sum_{i=1}^n
    g(\omega_i))^2
\le \frac{1}{n\epsilon}\|g\|^2_{L^\infty}. 
 \end{equation}
 
\end{proof}

This corollary implies that the set of $\omega_i$ where the estimate
$n^{-1}\sum_{i=1}^n g(\omega_i)$ is far from the desired value $\mathbb{E}g$
is small.

The practical usefulness of this algorithm depends upon the existence
of a \textit{repeatable} process (for instance some physical process)
which \textit{generates $\omega$ according to a desired distribution
  $\mu$}.

The precise meaning of this last statement is essentially that the
strong law of large numbers holds. Specifically, if
$\omega_1,...,\omega_n,...$ is a infinite sequence generated by the
process, and $A\subset \Omega$ is any a measurable set, then
\begin{equation}
 \lim_{n\rightarrow\infty} \frac{1}{n}\displaystyle\sum_{i=1}^n\chi_A(\omega_i) = \mu(A).
\end{equation}

Generating $n$ independent samples means generating
$\omega_1,...,\omega_n$ from $\mu^n$ according to the above notion.
The existence of a realizable process generating samples from a
probability distribution, and the practical use of such processes is
an interesting topic in the intersection of statistics, physics, and
computer science. In addition, statistics/probability theory studies
how to take samples from one probability distribution and transform
them to samples from another distribution.
