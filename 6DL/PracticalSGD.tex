\section{Practical Training Method
 in Large-Scale Machine Learning Problems}

\subsection{Basic Gradient Descent Type Algorithms}
Now we suppose we have a machine learning model
\begin{equation}
f(x; w),
\end{equation}
where $f$ can be any machine learning model like: linear regression, SVM or deep learning models. 

We have data as 
$$
z_i=\{x_i, y_i\}\quad i = 1:N,
$$
and the loss between the label and prediction is:
$$
f_i(w) = l(f(x_i, w), y_i),
$$
one example of $l$ is:
$$
l(f(x_i, w), y_i) = \|f(x_i, w)- y_i\|^2.
$$
What we want to solve is:
\begin{equation}
\mathop{\min}_{{w} \in \mathbb{R}^{n}} \frac{1}{N}\sum_{i=1}^N f_i(w).
\end{equation}

\subsubsection{Mini-Batch Method}
Now we introduce the Mini-Batch algorithm which is mostly used in basic algorithm for above problem:
\begin{algorithm}[H]
\caption{Mini-Batch}
\label{alg:mini-batch}
{\bf Input}: learning rate $\eta_t$, batch size $m$, parameter initialization $ w_0$, number of epochs $K$. \\
%For iteration $t = 1: K\frac{N}{m}$ \\
For Epoch $k = 1:K$\\
Shuffle data and get mini-batch $B_1, \cdots, B_{\frac{N}{m}}$, choose mini-batch as: $B_{i_t}$ with
$$
i_t \equiv t \mod(\frac{N}{m}),
$$
Compute the gradient on $B_{i_t}$:
$$
g_t = \nabla_{w} \frac{1}{m} \sum_{i \in B_{i_t}} f_i(w_{t})
$$
Update $w$:
\begin{equation}
w_{t+1} = w_t - \eta_t g_t.
\end{equation}
\end{algorithm}

This method is a little different from pure SGD type method as we still have
$$
\mathbb{E} g_t = \nabla f,
$$
except  the choice of $\nabla f_i$ and $\nabla f_{i+1}$ are dependent.
But this method works very well with some suitable batch-size. Recently, many researches show that small batch-size may lead to some flat minimizer with better generalization error. And some research suggests that, you can use a small batch size at the beginning and increase the batch size slowly with iteration.


\subsection{Deterministic Algorithm(a specially case):  Incremental Gradient Descent Method}
The cycle incremental gradient descent method:
\begin{algorithm}\caption{CIGD}
	\label{alg:CIGD}
	\begin{equation}\label{equ:GS-iteration}
	x_{t+1} = \Pi_{\mathcal{X}}(x_{t} - \eta_t \nabla f_{i_t}(x_t)), \quad t = 0:T,
	\end{equation}
	\begin{equation}
	i_t \equiv t \mod(m), \quad i_t \in 1:m,
	\end{equation}
\end{algorithm}


\subsection{Convergence for CIGD}
We start with the iteration scheme step by step, and analyze it first, and then sum a cycle iteration together.

A simple sufficient condition for the above assumption is that $f_i \in C^2(\mathcal{X})$ and is convex with $\mathcal{X}$ is a compact convex set. 

Here we prove an important lemma:
\begin{lemma}\label{lem:CIGDexpand}Let $\{x_k\}$ be the sequence generated by algorithm \ref{alg:CIGD} with $\eta_t = \eta_k$ for $t = km:((k+1)m-1)$, then for any $y \in \mathcal{X}$, we have
	\begin{equation}\label{equ:CIGDexpand}
	\|x_{(k+1)m} - y\|^2 \le \|x_{km} - y\|^2 - 2m\eta_k(f(x_{km}) - f(y)) + \eta_k^2\beta m^2M^2,
	\end{equation}
	where $\beta = 2 - \frac{1}{m}$
\end{lemma}

\begin{proof}
	By the definition of iteration scheme:
	\begin{equation}\label{equ:basicinequICGD}
	\|x_{km+j} - y\|^2 \le \|x_{km+j-1} - y\|^2 -2\eta_k \nabla f_{j}(x_{km+j-1})^{T}(x_{km+j-1}-y) + \eta_k^2 M^2, \quad \forall j = 1:m.
	\end{equation}
	Because of the convexity of $f_i$, we have
	\begin{equation}
	\nabla f_{j}(x_{km+j-1})^{T}(x_{km+j-1}-y) \ge f_{j}(x_{km+j-1}) - f_{j}(y).
	\end{equation}
	Add all $j = 1:m$ together
	\begin{equation}
	\|x_{(k+1)m} - y\|^2 \le \|x_{km} - y\|^2 -2m\eta_k (f(x_{km}) - f(y))+ m\eta_k^2 M^2 + 2\eta_k\sum_{j=1}^m(f_{j}(x_{km}) - f_{j}(x_{km+j-1}) ).
	\end{equation}
	For the last term we have:
	\begin{equation}
	f_{j}(x_{km}) - f_{j}(x_{km+j-1}) \le M\| x_{km} - x_{km + 1} \| + \cdots + M\|x_{km+j-2} - x_{km+j-1}\| \le 2(j-1)\eta_k M^2,
	\end{equation}
	the last inequality can be proven by take $y = x_{km + j -1}$ in \ref{equ:basicinequICGD}.
	
	Finally we have
	\begin{equation}
	\|x_{(k+1)m} - y\|^2 \le \|x_{km} - y\|^2 -2m\eta_k (f(x_{km}) - f(y))+ m\eta_k^2 M^2 + 2\eta_k^2 M^2m(m-1),
	\end{equation}
	which is the form of \ref{equ:CIGDexpand} with $\beta = 2 - \frac{1}{m}$.
\end{proof}

Using this lemma, we can have the next results:
\begin{theorem}
	Let $\eta_k$ be fixed at some positive constant $\eta$.
	\begin{itemize}
		\item If $ f^* = \min_{x \in \mathcal{X}}f(x) = -\infty$, then
		\begin{equation}
		\mathop{\lim\inf}_{k \to \infty} f(x_k) = f^*.
		\end{equation}
		\item If $f^* > -\infty$, then 
		\begin{equation}
		\mathop{\lim\inf}_{k \to \infty} f(x_k) \le f^* + \frac{\eta \beta m M^2}{2},
		\end{equation}
		where $\beta$ is the same in lemma \ref{lem:CIGDexpand}.
	\end{itemize}
\end{theorem}

\begin{theorem}
	If $X^* = \mathop{\arg\min}f(x)$ is nonempty. Then for $\epsilon > 0$, we have 
	\begin{equation}
	\min_{ 0\le k \le N} f(x_k) \le f^* + \frac{\eta \beta m^2 M^2 + \epsilon}{2m},
	\end{equation}
	where $N$ is given by
	\begin{equation}
	N = m \lfloor \frac{\rm{dist}(x_0;X^*)^2}{\eta \epsilon}\rfloor.
	\end{equation}
\end{theorem}

\begin{theorem}
	If the stepsize $\eta_k$ satisfy
	\begin{equation}
	\lim_{k \to \infty} \eta_k = 0, \quad \sum_{k=0}^{\infty} \eta_k = \infty.
	\end{equation}
	Then,
	\begin{equation}
	\mathop{\lim\inf}_{k\to \infty} f(x_k) = f^*.
	\end{equation}
	Furthermore, if $X^*$ is nonempty and 
	\begin{equation}
	\sum_{k=0}^\infty \eta_k^2 < \infty,
	\end{equation}
	then $\{x_k\}$ converges to some $x^* \in X^*$.
\end{theorem}

As far as we know, it is still an open problem to establish rate of
convergence for CIGD method.  In contrast, convergence rate in $L^2$
can be established for stochastic gradient (SGD) method.  
\subsection{Some comments}
Minimizing $f_1$ and $f_2$ (or $\sum_i f_i$) alternatively by using
gradient descent, this is the so-called ``cycle incremental gradient
method".   This kind of incremental methods are obviously closely
related to subspace correction method for both
stochastic and cycle types.  Such connections need to be further
investigated. 

With some assumption such as differentiability, convexity, Lipschitz
continuity, boundedness for gradient etc, qualitatively convergence
can be established for cycle type methods when diminishing stepsize
(or learning rate) is applied, but it is not known how convergence rate
can be established without randomization.

Some relevant discussions can be found in Bertsekas \cite{bertsekas2015convex, bertsekas2011incremental}
