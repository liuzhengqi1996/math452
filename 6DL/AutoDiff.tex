\chapter{AutoDiff}
\section{Introduction}
Suppose $\mathcal F:\mathbb U \mapsto \mathbb V$, (usually $\mathbb U = \mathbb R^n$ and $\mathbb V\in\mathbb R^m$) and the tangent space of a point $u\in\mathbb U$ with $v=\mathcal F(u) \in \mathbb V$ is $T_u\mathbb U$ and $T_v\mathbb V$. The dual of  $T_u\mathbb U$ and $T_v\mathbb V$ is  $(T_u\mathbb U)'$ and $(T_v\mathbb V)'$. Then for any $u\in\mathbb U$, $\mathcal{F}'(u)=\nabla \mathcal{F}(u)$ is the linear operator from $T_u\mathbb U$ to $T_v\mathbb V$ and $(\nabla \mathcal{F}(u))^T$ is the linear from $(T_v\mathbb V)'$ to $(T_u\mathbb U)'$.

 For any $u_0\in \mathbb U$, $v_0=\mathcal{F}(u_0)\in\mathbb V$, and assume the coat of time to calculate each entry of $\mathcal F$ or $\mathcal{F}'$ can be bounded by a constant, and let $F_{time}$ is the cost of calculating $\mathcal F$.
\begin{itemize}
	\item  Suppose $u$ is depend on a variable $s$. If we have already known $x^1=\dfrac{\partial u}{\partial s}(u_0)\in T_{u_0}\mathbb U$, the forward autodiff algorithm is to calculate
	\begin{equation}
	\dfrac{\partial v}{\partial s}(u_0)=	\dfrac{\partial \mathcal F}{\partial s}(u_0)=\nabla \mathcal F(u_0) \dfrac{\partial u}{\partial s}(u_0)=\nabla \mathcal F(u_0) x^1
	\end{equation}
	and the cost of time is $O(F_{time})$
 	\item  Suppose $t$ is a variable depend on $v$. If we have already known $y_1=(\dfrac{\partial t}{\partial v}(v_0))^T\in (T_{v_0}\mathbf V)'$ the backward autodiff algorithm is to calculate
 	\begin{equation}
 	(\dfrac{\partial t}{\partial u}(u_0))^T=(\dfrac{\partial v}{\partial u}(u_0))^T(\dfrac{\partial t}{\partial v}(v_0))^T=(\nabla \mathcal{F}(u_0))^T y_1
 	\end{equation}
 	and the cost of time is $O(F_{time})$
\end{itemize}

If we want to calculate $\nabla F$ by autodiff, we can set $x^1=e^1,...,e^n$ or set $y_1=e_1,...,e_m$, where $e^i$ is the $i$th column of $I_n$ and $e_i$ is the $i$th column of $I_m$. This means the cost of forward algorithm is $O(nF_{time})$ and the cost of backward algorithm is $O(mF_{time})$.

Now we have a series of $\mathcal F_j:\mathbb R^{n_j-1}\mapsto\mathbb R^{n_j}$, and $t=\mathcal{F}_J\circ\cdots\circ \mathcal{F}_1(s)$, if we use forward algorithm for all functional composition, the cost is $O(n_0(n_1+\cdots+n_J)F_{time})$, if we use backward  algorithm for all functional composition, the cost is $O(n_J(n_{J-1}+\cdots+n_0)F_{time})$. For deep learning problem, the $n_0$ is very large but $n_0=1$, so backward algorithm is much better.