%\newbreak
\section{KL divergence and cross-entropy}
Cross-entropy minimization is frequently used in optimization and rare-event probability estimation. When comparing a distribution    against a fixed reference distribution, cross-entropy and KL divergence are identical up to an additive constant. See more details in \cite{murphy2012machine,kullback1951information,kullback1997information} and the reference therein.

The KL(Kullback--Leibler) divergence defines a special distance between two discrete probability distributions 
$$
p=\left( \begin{array}{ccc}
p_1\\
\vdots \\
p_k
\end{array} \right),\quad  q=\left( \begin{array}{ccc}
q_1\\
\vdots \\
q_k
\end{array} \right)
$$
with $0\le p_i, q_i\le1$ and $\sum_{i=1}^{k}p_i=\sum_{i=1}^{k}q_i=1$ by
\begin{equation}
\label{KL-divergence}
D_{\rm KL}(q,p)= \sum_{i=1}^k q_i\log \frac{q_i}{p_i}.  
\end{equation}

\begin{lemma}\mbox{}
$D_{\rm KL}(q,p)$ works like a ``distance" without the symmetry:
	\begin{enumerate}
		\item $D_{\rm KL}(q,p)\ge0$;		
		\item $D_{\rm KL}(q,p)=0$ if and only if $p=q$;
	\end{enumerate}
\end{lemma}

\begin{proof}We first note that the elementary inequality
	\begin{equation}
	\log x \le x - 1, \quad\mathrm{for\ any\ }x\ge0,
	\end{equation}
	and the equality holds if and only if $x=1$.
	\begin{equation}
	-D_{\rm KL}(q,p) = - \sum_{i=1}^c q_i\log \frac{q_i}{p_i}   = \sum_{i=1}^k q_i\log \frac{p_i}{q_i} \le \sum_{i=1}^k q_i( \frac{p_i}{q_i}  - 1) = 0.
	\end{equation}
	And the equality holds if and only if 
	\begin{equation}
	\frac{p_i}{q_i} = 1 \quad \forall i = 1:k.
	\end{equation}
\end{proof}

Define cross-entropy for distribution $p$ and $q$ by
\begin{equation}\label{Cross-Entropy}
H(q,p) = - \sum_{i=1}^k q_i \log p_i,
\end{equation}
and the entropy for distribution $q$ by 
\begin{equation}\label{Entropy}
H(q) = - \sum_{i=1}^k q_i \log q_i.
\end{equation}
Note that
\begin{equation}
D_{\rm KL}(q,p)= \sum_{i=1}^k q_i\log \frac{q_i}{p_i} =  \sum_{i=1}^k q_i \log q_i - \sum_{i=1}^k q_i \log p_i
\end{equation}
Thus, 
\begin{equation}\label{KLandEntropy}
H(q,p) = H(q) + D_{\rm KL}(q,p).
\end{equation} 
It follows from the relation \eqref{KLandEntropy} that
\begin{equation}
\label{EntropyandKL}
\mathop{\arg\min}_p D_{\rm KL}(q,p)=\mathop{\arg\min}_p H(q,p).
\end{equation}

The concept of cross-entropy  can be used to define a loss function in machine learning and optimization. 
Let us assume $y_i$ is the true label for $x_i$, for example $y_i = e_{k_i}$ if $x_i \in A_{k_i}$. Consider the predicted distribution 
\begin{equation}\label{key}
\bm p(x; \bm \theta) = \frac{1}{\sum\limits_{i=1}^k e^{w_i x+b_i}}
\begin{pmatrix}
e^{w_1 x+b_1}\\
e^{w_2 x+b_2}\\
\vdots\\
e^{w_k x+b_k}
\end{pmatrix}
= \begin{pmatrix}
p_1(x; \bm\theta) \\
p_2(x; \bm\theta) \\
\vdots \\
p_k(x; \bm\theta)
\end{pmatrix}
\end{equation}
for any data $x \in A$.
By \eqref{EntropyandKL}, the minimization of KL divergence is equivalent to the minimization of the cross-entropy, namely
\begin{equation}
\mathop{\arg\min}_{\theta} \sum_{i=1}^N D_{\rm KL}(y_i,\bm p(x_i; \bm \theta)) = \mathop{\arg\min}_{\theta} \sum_{i=1}^N H(y_i, \bm p(x_i; \bm \theta)).
\end{equation} 
Recall that we have all data $D = \{(x_1,y_1),(x_2,y_2),\cdots, (x_N, y_N)\}$.  Then, it is natural to consider the 
loss function as following:
\begin{equation}
\sum_{j=1}^N H(y_i, \bm p(x_i; \bm \theta)),
\end{equation}
which measures the distance between the real label and predicted one for all data.
In the meantime, we can check that
\begin{equation}
\begin{aligned}
\sum_{j=1}^N H(y_j, \bm p(x_j; \bm \theta))&=-\sum_{j=1}^N y_j  \cdot \log  \bm p(x_j; \bm \theta )\\
&=-\sum_{j=1}^N  \log p_{i_j}(x_i; \bm \theta) \quad (\text{because}~y_j = e_{i_j}~\text{for}~x_j \in A_{i_j})\\
&=-\sum_{i=1}^k \sum_{x\in A_i}  \log p_{i}(x; \bm \theta) \\
&=-\log \prod_{i=1}^k \prod_{x\in A_i}   p_{i}(x; \bm \theta)\\
& = L(\theta)
\end{aligned}
\end{equation}
with $L(\theta)$ defined in \eqref{logistic} as 
\begin{equation}
L(\bm \theta) = - \sum_{i=1}^k \sum_{x\in A_i} \log p_{i}(x;\bm \theta).
\end{equation}

That is to say, the logistic regression loss function defined by likelihood in \eqref{logistic} is exact
the loss function defined by measuring the distance between real label and predicted one via cross-entropy.
We can note 
\begin{equation}\label{key}
\min_{\bm \theta} L_\lambda(\bm \theta) \Leftrightarrow \min_{\bm \theta} \sum_{j=1}^N H(y_i, \bm p(x_i; \bm \theta)) + \lambda R(\|\bm \theta\|) 
\Leftrightarrow \min_{\bm \theta} \sum_{j=1}^N D_{\rm KL}(y_i, \bm p(x_i; \bm \theta)) + \lambda R(\|\bm \theta\|).
\end{equation}



\endinput