\chapter{Relationship between SA and SGD (Minibatch)}
\section{From SGD (minibatch) to SA}
We first consider the following lost function 
$$ 
F(x) = \frac{1}{N} \sum_{i=1}^N f_i(x),
$$ 
where $N$ is the number of data point. The Minibatch SGD is written
as 
\begin{equation} \label{equ:minibatch}
x^{n+1} = x^n - \Delta t_{n} \nabla F_{B_n}(x^n),
\end{equation}
where 
$$ 
F_{B_n}(x) = \frac{1}{|B_n|} \sum_{j \in B_n} f_j(x).
$$ 
Now, we rewrite \eqref{equ:minibatch} as 
\begin{equation} \label{equ:minibach-SA}
X^{n+1} = X^n - \Delta t_{n} \nabla F(X^n) + \Delta {t}_n
\underbrace{[\nabla F(X^n) - \nabla F_{B_n}(X^n)]}_{Y^n}. 
\end{equation}

\begin{equation} \label{equ:minibach-SA}
X^{n+1} = X^n - a_{n} \nabla F(X^n) + a_n
[\nabla F(X^n) - \nabla F_{B_n}(X^n)-\frac{b_n}{a_n}] + b_nW_n 
\end{equation}
\begin{equation} \label{equ:minibach-SA}
X^{n+1} = X^n - a_{n} [\nabla F(X^n) + \xi_n] + b_nW_n 
\end{equation}
where
$$
a_n=\Delta t_n, \xi_n=\nabla F(X^n) - \nabla F_{B_n}(X^n)-\frac{b_n}{a_n}W_n.
$$

Let $p_i$ is the probability of choosing the $i$-th component to
the minibatch. We note here that each component may be in the
minibatch for more than once. Then, we have 
$$ 
\begin{aligned}
\mathbb{E}(Y^n) &= \nabla F(X^n) - \mathbb{E} [\nabla F_{B_n}(X^n)] \\
&= \frac{1}{N} \sum_{i=1}^N \nabla f_i(X^n) - \frac{1}{|B_n|}
\mathbb{E}[\sum_{j\in B_n} \nabla f_j(X^n)] \\
&= \frac{1}{N} \sum_{i=1}^N \nabla f_i(X^n) - \mathbb{E}[\nabla
f(X^n)] \qquad (\text{independence}) \\
&= \frac{1}{N} \sum_{i=1}^N \nabla f_i(X^n) - \sum_{i=1}^N p_i \nabla
f_i(X^n).
\end{aligned}
$$ 
Therefore, we have the following condition
\begin{equation} \label{equ:mean-condition}
\frac{1}{N}\sum_{i=1}^N \nabla f_i(x^n) = \mathbb{E}[\nabla f(X^n)].
\end{equation}
From the above condition, a simple choice is $p_i = \frac{1}{N}$,
namely uniform distribution. Condition \eqref{equ:mean-condition}
can also be written as $\mathbb{E}(Y^n) = 0$.

Note that ${\rm Var}(Y^n) = \mathbb{E}[Y^n (Y^n)^T] - \mathbb{E}(Y^n)
\mathbb{E}(Y^n)^T = \mathbb{E}[Y^n (Y^n)^T]$. For simplicity,
we remove the dependence of $X^n$ in the following. Then, 
$$ 
\begin{aligned}
{\rm Var}(Y^n) &= \mathbb{E}[ (\nabla F - \nabla F_{B_n})
(\nabla F - \nabla F_{B_n})^T ]
\\
&= \mathbb{E}[\nabla F_{B_n} (\nabla F_{B_n})^T ] - \nabla F (\nabla
    F)^T \\
&= \frac{1}{|B_n|^2} \mathbb{E}_{i_1} \mathbb{E}_{i_1} \cdots
\mathbb{E}_{i_{|B_n|}}[ (\sum_{j=1}^{|B_n|} \nabla f_{i_j})
  (\sum_{j=1}^{|B_n|} \nabla f_{i_j})^T ] - \nabla F (\nabla F)^T \\
&= \frac{1}{|B_n|^2} \mathbb{E}_{i_1} \mathbb{E}_{i_2} \cdots
\mathbb{E}_{i_{|B_n|}}[ \sum_{j,r=1 , i\neq j}^{|B_n|} \nabla f_{i_j}
(\nabla f_{i_r})^T] \\ 
& ~~ + \frac{1}{|B_n|^2} \mathbb{E}_{i_1} \mathbb{E}_{i_2} \cdots
\mathbb{E}_{i_{|B_n|}} [\sum_{j=1}^{|B_n|} \nabla
f_{i_j} (\nabla f_{i_j})^T]- \nabla F (\nabla F)^T \\ 
& = (1 - \frac{1}{|B_n|}) \mathbb{E}(\nabla f) [\mathbb{E}(\nabla
    f)]^T + \frac{1}{|B_n|} \mathbb{E}[ (\nabla f) (\nabla f)^T] -
\nabla F (\nabla F)^T \\
&= \frac{1}{|B_n|} \left( \mathbb{E}[(\nabla f) (\nabla f)^T] -
    \mathbb{E}(\nabla f) [\mathbb{E}(\nabla f)^T] 
    \right) \\
&= \frac{1}{|B_n|} {\rm Var}(\nabla f).
\end{aligned}
$$ 

\begin{remark}
Since the index here can be repeated, there is NO need that ${\rm
Var}(Y^n) = 0$ when $|B_n| = N$.
\end{remark}

In summary, we have 
\begin{equation} \label{equ:summary}
\mathbb{E}(Y^n) = 0, \qquad {\rm Var}(Y^n) = \frac{1}{|B_n|} {\rm
  Var}(\nabla f).
\end{equation}
It is conceivable that the result can be extended to the expectation
version of the lost function, namely, 
$$ 
F(x) = \mathbb{E}_{\xi} f(x, \xi).
$$ 


\section{Convergence result}
In the SA scheme, 
\begin{equation} \label{equ:SA}
X^{n+1} = X^n - \Delta t_n \nabla F(X^n) + \sigma_n W_{\Delta t_n},
\end{equation}
the convergence is proven under the following condition: 
$$ 
\Delta t_n = \mathcal{O}(n), \qquad \sigma_n^2 =
\mathcal{O}(\frac{1}{n \log\log n})
$$ 
Compare the variance of $\sigma_n W_{\Delta t_n}$ and $\Delta t_n
Y^n$, we have 
$$ 
\sigma_n^2 \Delta t_n \approx \Delta t_n^2 {\rm Var}(Y^n),
$$ 
which implies that 
$$ 
\frac{1}{|B_n|} {\rm Var}(\nabla f) =
\mathcal{O}(\frac{\sigma_n^2}{\Delta t_n}) =
\mathcal{O}(\frac{1}{\log\log n}).
$$ 
We conclude that, if the SGD is stagnated, namely ${\rm Var}(\nabla
f)$ stays the almost the same, we need to slowly increase the size of
minibatch in the way that 
\begin{equation} \label{equ:batch-size}
|B_n| = \mathcal{O}(\log\log n).
\end{equation}


\begin{remark}
The above argument requires that $\nabla f(x, \xi)$ is Gaussian on
$\xi$ for any $x$. This assumption holds for linear Gaussian
regression. 
\end{remark}



