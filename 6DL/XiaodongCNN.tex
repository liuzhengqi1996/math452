\section{Xiaodong's Examples on CIFAR data set}
Here are some examples of CNN, with specific choice of hyperparameters, when it is applied to CIFAR data sets. 

\subsection{TOP-1}
What's the best accuracy that can be achieved?  94\%?  99\%?

The best two results on CIFAR-10 are as follows.
\begin{enumerate}
    \item $99.0\%$, \emph{GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism}\footnote{\emph{GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism}, \url{https://arxiv.org/pdf/1811.06965.pdf}} (GPipe)
    \item $98.52\%$, \emph{AutoAugment: Learning Augmentation Policies from Data}\footnote{\emph{AutoAugment: Learning Augmentation Policies from Data}, \url{https://arxiv.org/pdf/1805.09501.pdf}} (AutoAug)
\end{enumerate}

\subsection{GPipe}
GPipe first trains a model (a 557 million parameters AmoebaNet) on ImageNet, then fine-tunes the trained model on CIFAR-10. GPipe is put forward as a parallelism library for training really large models.

\subsection{AutoAug}
AutoAug is method that leans data augmentation strategies from Data. Currently, there are many operations on images, such as shearing or distortion. AutoAug picks 16 operations, and the aim is to find the best combination of them. A combination is called a policy, which contains 5 sub-policies. For each image in a mini-batch, one of the sub-policies is chosen randomly (in a uniform distribution) to apply on the image. The transformed image is then used for training.

To gain the result, the model PyramidNet\footnote{\emph{Deep Pyramidal Residual Networks}, \url{https://arxiv.org/abs/1610.02915}} and the regularization method ShakeDrop\footnote{\emph{ShakeDrop Regularization for Deep Residual Learning
}, \url{https://arxiv.org/abs/1802.02375}} are used.  
Until May 2019, ShakeDrop achieves the third best accuracy ($97.69\%$) on CIFAR-10, which also uses PyramidNet. So the model and the regularization method used in AutoAug are basically from the ShakeDrop paper. AutoAug in fact provides an additional data enhancement strategy to improve ShakeDrop.

The policy in AutoAug is generated by an RNN. Since each policy affects the accuracy of the trained model, the accuracy can be used (as a reward) to update the weights of RNN, so as to make the RNN generates a better policy. Thus, AutoAug solves an reinforcement learning problem: the agent\footnote{T o avoid confusion, we will use \emph{agent} instead of \emph{policy} here, since \emph{policy} has been used in AutoAug.}  is an RNN, the reward function is the validation accuracy, and the optimization method is policy gradient.

\begin{enumerate}
    \item Action space: the policy space. A policy contains 5 sub-policies, and each sub-policy contains two operations (they will be applied in order). For each operation, there are its type (one of the 16 types), its magnitude (discretized to 10 values), and its probability to use (discretized to 11 values). The total number of different policies are thus $(16\times10\times11)^{2\times5}\approx2.8\times10^{32}$.
    \item Agent (or the controller): an RNN\footnote{The training procedure follows from NASNet (\emph{Learning Transferable Architectures for Scalable Image Recognition }, \url{https://arxiv.org/abs/1707.07012}), which is based on NAS (\emph{Neural Architecture Search with Reinforcement Learning }, \url{https://arxiv.org/abs/1611.01578}). Originally, RL is used to search the best architecture of neural networks. Later, the architecture space is replaced with data enhancement policy space, and RL is used to search the best policy.}. The RNN provides the distribution of the action, i.e. the policy. On each dataset, the agent samples about $15,000$ policies. At each step, the controller RNN of AutoAug predicts a decision produced by a softmax; the prediction is then fed into the next step as an embedding. In total the controller has 30 softmax predictions in order to predict 5 sub-policies, each with 2 operations, and each operation requiring an operation type, magnitude and probability.
    \item Reward: the validation accuracy. For each policy, the model is first trained with the policy to convergence, and then the validation accuracy is computed.
    \item Optimization method: policy gradient. Proximal policy optimization (PPO) is used.
\end{enumerate}

 

