\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\citation{mitchell1997machine}
\citation{russell2010artificial}
\citation{friedman2001elements}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Machine Learning and Image Classification}{7}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction to machine learning}{7}{section.1.1}\protected@file@percent }
\citation{wolpert1996lack}
\citation{goodfellow2016deep}
\citation{praveena2017literature}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}A basic machine learning problem: image classification}{8}{section.1.2}\protected@file@percent }
\citation{lecun1998mnist}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Some popular data sets in image classification}{10}{section.1.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Basic descriptions about popular datasets }}{10}{table.1.1}\protected@file@percent }
\newlabel{popular_dataset}{{1.1}{10}{Basic descriptions about popular datasets}{table.1.1}{}}
\citation{krizhevsky2009learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}MNIST (Modified National Institute of Standards and Technology Database)}{11}{subsection.1.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Some images in MNIST.}}{11}{figure.1.1}\protected@file@percent }
\newlabel{A2}{{1.1}{11}{MNIST (Modified National Institute of Standards and Technology Database)}{equation.1.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}CIFAR}{11}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{CIFAR-10}{11}{section*.3}\protected@file@percent }
\citation{krizhevsky2009learning}
\citation{deng2009imagenet}
\newlabel{Fig: CIFAR-10}{{1.3.2}{12}{CIFAR-10}{section*.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Left: Some images in CIFAR-10. Right: Some images in CIFAR-100.}}{12}{figure.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{CIFAR-100}{12}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}ImageNet}{12}{subsection.1.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Some images in ImageNet.}}{13}{figure.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Linear Machine Learning Models}{15}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Definition of linearly separable sets}{15}{section.2.1}\protected@file@percent }
\newlabel{lem:2class}{{1}{15}{Definition of linearly separable sets}{definition.1}{}}
\newlabel{2classH}{{2.1}{15}{Definition of linearly separable sets}{equation.2.1.1}{}}
\newlabel{lem:2class}{{1}{15}{Definition of linearly separable sets}{lemma.1}{}}
\newlabel{Wb}{{2.2}{15}{Definition of linearly separable sets}{equation.2.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces One linearly separable set}}{16}{figure.2.1}\protected@file@percent }
\newlabel{twoclassification}{{2.1}{16}{One linearly separable set}{figure.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Two non-linearly separable sets}}{16}{figure.2.2}\protected@file@percent }
\newlabel{twoclassification}{{2.2}{16}{Two non-linearly separable sets}{figure.2.2}{}}
\newlabel{eq:3}{{2.3}{16}{Definition of linearly separable sets}{equation.2.1.3}{}}
\newlabel{eq:3}{{2.4}{16}{Definition of linearly separable sets}{equation.2.1.4}{}}
\newlabel{Wb}{{2.6}{16}{Definition of linearly separable sets}{equation.2.1.6}{}}
\newlabel{eq:3}{{2.7}{16}{Definition of linearly separable sets}{equation.2.1.7}{}}
\newlabel{Hij}{{2.8}{17}{Definition of linearly separable sets}{equation.2.1.8}{}}
\newlabel{Interplation}{{2}{17}{Definition of linearly separable sets}{lemma.2}{}}
\newlabel{Gammai}{{2.9}{17}{Definition of linearly separable sets}{equation.2.1.9}{}}
\newlabel{AiGamma}{{2.10}{17}{Definition of linearly separable sets}{equation.2.1.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Linearly separable sets in 2-d space (k = 3)}}{17}{figure.2.3}\protected@file@percent }
\newlabel{twoclassification}{{2.3}{17}{Linearly separable sets in 2-d space (k = 3)}{figure.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces All-vs-One linearly separable sets (k = 3)}}{17}{figure.2.4}\protected@file@percent }
\newlabel{twoclassification}{{2.4}{17}{All-vs-One linearly separable sets (k = 3)}{figure.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Pairwise linearly separable sets in 2-d space (k = 3)}}{18}{figure.2.5}\protected@file@percent }
\newlabel{pairwise_separable_example}{{2.5}{18}{Pairwise linearly separable sets in 2-d space (k = 3)}{figure.2.5}{}}
\citation{gelman2006data}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Introduction to logistic regression}{20}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Logistic regression}{20}{subsection.2.2.1}\protected@file@percent }
\newlabel{sec:LR}{{2.2.1}{20}{Logistic regression}{subsection.2.2.1}{}}
\newlabel{softmax}{{5}{20}{Logistic regression}{definition.5}{}}
\newlabel{key}{{2.16}{20}{Logistic regression}{equation.2.2.16}{}}
\newlabel{key}{{2.19}{21}{Logistic regression}{equation.2.2.19}{}}
\newlabel{key}{{2.20}{21}{Logistic regression}{equation.2.2.20}{}}
\newlabel{lemm:H1/2}{{5}{21}{Logistic regression}{lemma.5}{}}
\newlabel{thm2}{{7}{22}{Logistic regression}{lemma.7}{}}
\newlabel{key}{{2.30}{22}{Logistic regression}{equation.2.2.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Regularized logistic regression}{23}{subsection.2.2.2}\protected@file@percent }
\newlabel{key}{{2.31}{23}{Regularized logistic regression}{equation.2.2.31}{}}
\newlabel{key}{{2.32}{23}{Regularized logistic regression}{equation.2.2.32}{}}
\newlabel{key}{{2.33}{23}{Regularized logistic regression}{equation.2.2.33}{}}
\newlabel{key}{{2.34}{23}{Regularized logistic regression}{equation.2.2.34}{}}
\newlabel{thm-L-Theta}{{1}{23}{Regularized logistic regression}{theorem.2.2.1}{}}
\newlabel{key}{{2.35}{23}{Regularized logistic regression}{equation.2.2.35}{}}
\citation{murphy2012machine,kullback1951information,kullback1997information}
\newlabel{key}{{2.36}{24}{Regularized logistic regression}{equation.2.2.36}{}}
\newlabel{eq:logisticlambda}{{2.38}{24}{Regularized logistic regression}{equation.2.2.38}{}}
\newlabel{logistic}{{2.39}{24}{Regularized logistic regression}{equation.2.2.39}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Logistic Regression}}{24}{algorithm.1}\protected@file@percent }
\newlabel{alg:LR-R}{{1}{24}{Regularized logistic regression}{algorithm.1}{}}
\newlabel{key}{{2.40}{24}{Regularized logistic regression}{equation.2.2.40}{}}
\newlabel{key}{{2.41}{24}{Regularized logistic regression}{equation.2.2.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}KL divergence and cross-entropy}{25}{section.2.3}\protected@file@percent }
\newlabel{KL-divergence}{{2.42}{25}{KL divergence and cross-entropy}{equation.2.3.42}{}}
\newlabel{Cross-Entropy}{{2.46}{25}{KL divergence and cross-entropy}{equation.2.3.46}{}}
\newlabel{Entropy}{{2.47}{25}{KL divergence and cross-entropy}{equation.2.3.47}{}}
\newlabel{KLandEntropy}{{2.49}{26}{KL divergence and cross-entropy}{equation.2.3.49}{}}
\newlabel{EntropyandKL}{{2.50}{26}{KL divergence and cross-entropy}{equation.2.3.50}{}}
\newlabel{key}{{2.51}{26}{KL divergence and cross-entropy}{equation.2.3.51}{}}
\citation{drucker1997support,ben2001support,cortes1995support,cristianini2000introduction}
\newlabel{key}{{2.56}{27}{KL divergence and cross-entropy}{equation.2.3.56}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Support vector machine}{27}{section.2.4}\protected@file@percent }
\newlabel{sec:SVMintro}{{2.4}{27}{Support vector machine}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Binary SVM}{27}{subsection.2.4.1}\protected@file@percent }
\newlabel{sec:SVM}{{2.4.1}{27}{Binary SVM}{subsection.2.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces SVM}}{28}{figure.2.6}\protected@file@percent }
\newlabel{fig:margin}{{2.6}{28}{SVM}{figure.2.6}{}}
\newlabel{binarySVM}{{2.60}{28}{Binary SVM}{equation.2.4.60}{}}
\newlabel{maxSVM}{{2.70}{29}{Binary SVM}{equation.2.4.70}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Soft margin maximization and kernel methods}{30}{subsection.2.4.2}\protected@file@percent }
\newlabel{SVM_Quad}{{2.79}{30}{Soft margin maximization and kernel methods}{equation.2.4.79}{}}
\newlabel{SVM_Quad_soft}{{2.81}{31}{Soft margin maximization and kernel methods}{equation.2.4.81}{}}
\newlabel{SVM_soft}{{2.85}{31}{Soft margin maximization and kernel methods}{equation.2.4.85}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Binary logistic regression}{32}{subsection.2.4.3}\protected@file@percent }
\newlabel{key}{{2.95}{33}{Binary logistic regression}{equation.2.4.95}{}}
\newlabel{key}{{2.97}{33}{Binary logistic regression}{equation.2.4.97}{}}
\newlabel{key}{{2.101}{33}{Binary logistic regression}{equation.2.4.101}{}}
\newlabel{binaryLR}{{2.102}{34}{Binary logistic regression}{equation.2.4.102}{}}
\citation{rosset2004margin}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Probability}{37}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction to probability}{37}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Basic probability}{37}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Basic Probability Theory}{37}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Discrete Examples}{37}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Independent Copies}{38}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Continuous Distributions}{38}{subsection.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Gaussian/ Normal Distribution}{39}{subsection.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Random, Variable, Mean, Variance}{40}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Random Variable}{40}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Mean of random variable}{41}{subsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Variance of Random Variables}{41}{subsection.3.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Independenve of Random variables}{41}{subsection.3.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}Properties of E, V, Independence}{42}{subsection.3.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Probability interpretation of logistic regression}{42}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Logistic Regression Model}{42}{subsection.3.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Learning the parameters $\ensuremath  {\mathchoice {\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$\displaystyle \mathbf  {a}$}} {\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$\textstyle \mathbf  {a}$}} {\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$\scriptstyle \mathbf  {a}$}} {\unhbox \voidb@x \hbox {\relax \mathversion  {bold}$\scriptscriptstyle \mathbf  {a}$}}}, b$ from data }{43}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Maximamum Likelihood}{44}{section.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Basic Statistical Learning Theory}{44}{section.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Maximum Likelihood Estimate(MLE)}{45}{subsection.3.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Classfication/ Logistic Regression}{45}{section.3.8}\protected@file@percent }
\newlabel{eq:pp}{{3.1}{46}{Classfication/ Logistic Regression}{equation.3.8.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Bayesian Approach to Machine Learning}{46}{section.3.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.1}Goal}{46}{subsection.3.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.2}Example: Image Classification/ Logistic Regression}{48}{subsection.3.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.10}General Covariance}{48}{section.3.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.10.1}Whitening}{49}{subsection.3.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.10.2}Batch Normalization}{49}{subsection.3.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.10.3}Central Limiting Theorem}{49}{subsection.3.10.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Training Algorithms}{51}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Line search and gradient descent method}{51}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Gradient descent method}{51}{subsection.4.1.1}\protected@file@percent }
\newlabel{optmodel}{{4.1}{51}{Gradient descent method}{equation.4.1.1}{}}
\@writefile{toc}{\contentsline {paragraph}{A general approach: line search method}{51}{section*.5}\protected@file@percent }
\newlabel{line-search}{{4.2}{52}{A general approach: line search method}{equation.4.1.2}{}}
\newlabel{key}{{4.3}{52}{A general approach: line search method}{equation.4.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Negative Gradient Direction: $ x_t$ is current point, $ p_t$ is the negative gradient of $ x_t$, i.e., $- \nabla f( x_t)$.}}{53}{figure.4.1}\protected@file@percent }
\newlabel{functiongradient}{{4.1}{53}{Negative Gradient Direction: $ x_t$ is current point, $ p_t$ is the negative gradient of $ x_t$, i.e., $- \nabla f( x_t)$}{figure.4.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Gradient Descent Method}}{53}{algorithm.2}\protected@file@percent }
\newlabel{alg:LR-R}{{2}{53}{A general approach: line search method}{algorithm.2}{}}
\newlabel{key}{{4.5}{53}{A general approach: line search method}{equation.4.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Convergence of Gradient Descent method}{54}{subsection.4.1.2}\protected@file@percent }
\newlabel{key}{{4.6}{54}{Convergence of Gradient Descent method}{equation.4.1.6}{}}
\newlabel{key}{{4.7}{54}{Convergence of Gradient Descent method}{equation.4.1.7}{}}
\newlabel{key}{{4.8}{55}{Convergence of Gradient Descent method}{equation.4.1.8}{}}
\newlabel{key}{{4.9}{55}{Convergence of Gradient Descent method}{equation.4.1.9}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces  FGD}}{55}{algorithm.3}\protected@file@percent }
\newlabel{alg:FGD}{{3}{55}{Convergence of Gradient Descent method}{algorithm.3}{}}
\newlabel{equ:fgd-iteration}{{4.10}{55}{Convergence of Gradient Descent method}{equation.4.1.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Stochastic gradient descent method and convergence theory}{56}{section.4.2}\protected@file@percent }
\newlabel{SGDproblem}{{4.16}{56}{Stochastic gradient descent method and convergence theory}{equation.4.2.16}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces SGD}}{56}{algorithm.4}\protected@file@percent }
\newlabel{alg:SGD}{{4}{56}{Stochastic gradient descent method and convergence theory}{algorithm.4}{}}
\newlabel{equ:sgd-iteration}{{4.17}{56}{Stochastic gradient descent method and convergence theory}{equation.4.2.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Convergence of SGD}{56}{subsection.4.2.1}\protected@file@percent }
\newlabel{key}{{4.18}{56}{Convergence of SGD}{equation.4.2.18}{}}
\newlabel{equ:L2SGD}{{4.20}{57}{Convergence of SGD}{equation.4.2.20}{}}
\newlabel{key}{{4.21}{57}{Convergence of SGD}{equation.4.2.21}{}}
\newlabel{key}{{4.23}{57}{Convergence of SGD}{equation.4.2.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}SGD with mini-batch}{58}{subsection.4.2.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces SGD with mini-batch}}{58}{algorithm.5}\protected@file@percent }
\newlabel{alg:SGD}{{5}{58}{SGD with mini-batch}{algorithm.5}{}}
\newlabel{equ:sgd-iteration}{{4.25}{58}{SGD with mini-batch}{equation.4.2.25}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Shuffle SGD with mini-batch}}{58}{algorithm.6}\protected@file@percent }
\newlabel{alg:mini-batch}{{6}{58}{SGD with mini-batch}{algorithm.6}{}}
\newlabel{key}{{4.26}{59}{SGD with mini-batch}{equation.4.2.26}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Polynomials and Weierstrass theorem}{61}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Weierstrass Theorem}{61}{section.5.1}\protected@file@percent }
\newlabel{intq}{{5.4}{62}{Weierstrass Theorem}{equation.5.1.4}{}}
\newlabel{qest}{{5.7}{62}{Weierstrass Theorem}{equation.5.1.7}{}}
\newlabel{fcont}{{5.8}{62}{Weierstrass Theorem}{equation.5.1.8}{}}
\newlabel{Qn}{{5.12}{62}{Weierstrass Theorem}{equation.5.1.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Curse of dimensionality}{63}{subsection.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Runge's phenomenon}{63}{subsection.5.1.2}\protected@file@percent }
\citation{strichartz2003guide}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Runge's phenomenon: Runge function $f(x)=\frac  {1}{1+25x^{2}}$ and its polynomial interpolation $p_n(x)$.}}{64}{figure.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Fourier transform and Fourier series}{64}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Fourier transform}{64}{subsection.5.2.1}\protected@file@percent }
\newlabel{def:schwarz}{{7}{64}{Fourier transform}{definition.7}{}}
\newlabel{def:fourier1}{{9}{65}{Fourier transform}{definition.9}{}}
\newlabel{def:fourier2}{{10}{65}{Fourier transform}{definition.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Poisson summation formula}{66}{subsection.5.2.2}\protected@file@percent }
\citation{johnson2015saddle}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}A special cut-off function}{67}{subsection.5.2.3}\protected@file@percent }
\newlabel{alpha-g}{{5.13}{67}{A special cut-off function}{equation.5.2.13}{}}
\newlabel{eq_181}{{5.14}{67}{A special cut-off function}{equation.5.2.14}{}}
\newlabel{gaussInt}{{5.16}{68}{A special cut-off function}{equation.5.2.16}{}}
\citation{strichartz2003guide}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Fourier transform of polynomials}{69}{subsection.5.2.4}\protected@file@percent }
\newlabel{sigma-epsilon}{{5.19}{69}{Fourier transform of polynomials}{equation.5.2.19}{}}
\newlabel{eq_278}{{5.20}{69}{Fourier transform of polynomials}{equation.5.2.20}{}}
\newlabel{polynomial_lemma}{{19}{69}{Fourier transform of polynomials}{lemma.19}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Finite Element Method}{71}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Linear finite element spaces}{71}{section.6.1}\protected@file@percent }
\newlabel{FEspace}{{6.1}{71}{Linear finite element spaces}{section.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Triangulations}{71}{subsection.6.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces 1D uniform grid}}{71}{figure.6.1}\protected@file@percent }
\newlabel{fig:1dpartition}{{6.1}{71}{1D uniform grid}{figure.6.1}{}}
\newlabel{partitionyx}{{6.1}{72}{Triangulations}{equation.6.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces 2D grids}}{72}{figure.6.2}\protected@file@percent }
\newlabel{2duniform}{{6.2}{72}{2D grids}{figure.6.2}{}}
\newlabel{shape}{{6.2}{72}{Triangulations}{equation.6.1.2}{}}
\newlabel{A3.1}{{6.3}{72}{Triangulations}{equation.6.1.3}{}}
\newlabel{A3.2}{{6.4}{72}{Triangulations}{equation.6.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Geometric explanation of barycentric coordinates}}{73}{figure.6.3}\protected@file@percent }
\newlabel{fig:barycentricCoor}{{6.3}{73}{Geometric explanation of barycentric coordinates}{figure.6.3}{}}
\newlabel{eq:lambdasolution}{{6.6}{73}{Triangulations}{equation.6.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Continuous linear finite element spaces}{74}{subsection.6.1.2}\protected@file@percent }
\newlabel{linearFE}{{6.1.2}{74}{Continuous linear finite element spaces}{subsection.6.1.2}{}}
\newlabel{LinFE}{{6.7}{74}{Continuous linear finite element spaces}{equation.6.1.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Nodal basis functions and dual basis}{74}{section*.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Dual basis functions of $V_h$ in 1D for $n_h=5$.}}{74}{figure.6.4}\protected@file@percent }
\newlabel{fig:dualbasis}{{6.4}{74}{Dual basis functions of $V_h$ in 1D for $n_h=5$}{figure.6.4}{}}
\newlabel{eq:1}{{6.8}{75}{Nodal basis functions and dual basis}{equation.6.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Nodal basis functions in 1d and 2d}}{75}{figure.6.5}\protected@file@percent }
\newlabel{fig:nodalbasis}{{6.5}{75}{Nodal basis functions in 1d and 2d}{figure.6.5}{}}
\citation{xu1982estimate}
\citation{xu2013estimate}
\newlabel{1dbasis:function}{{6.9}{76}{Nodal basis functions and dual basis}{equation.6.1.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Plot of a typical element from $V_{h,0}$.}}{76}{figure.6.6}\protected@file@percent }
\newlabel{fig:1dtypical}{{6.6}{76}{Plot of a typical element from $V_{h,0}$}{figure.6.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Nodal value interpolant}{76}{section*.7}\protected@file@percent }
\newlabel{sc-seio}{{6.1.2}{76}{Nodal value interpolant}{section*.7}{}}
\newlabel{u-interp}{{6.10}{76}{Nodal value interpolant}{equation.6.1.10}{}}
\newlabel{interp00}{{9}{76}{Nodal value interpolant}{theorem.6.1.9}{}}
\newlabel{error0}{{6.11}{76}{Nodal value interpolant}{equation.6.1.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Approximation of finite element space.}}{77}{figure.6.7}\protected@file@percent }
\newlabel{Interpolation}{{6.7}{77}{Approximation of finite element space}{figure.6.7}{}}
\newlabel{gpp}{{6.12}{77}{Nodal value interpolant}{equation.6.1.12}{}}
\newlabel{Taylor_vi}{{6.13}{77}{Nodal value interpolant}{equation.6.1.13}{}}
\citation{scott1990finite}
\citation{Xu.J2015a}
\newlabel{Ihvv}{{6.14}{78}{Nodal value interpolant}{equation.6.1.14}{}}
\newlabel{interp0}{{10}{79}{Nodal value interpolant}{theorem.6.1.10}{}}
\newlabel{error0N}{{6.15}{79}{Nodal value interpolant}{equation.6.1.15}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Deep Neural Network Functions}{81}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Motivation: from finite element to neural network}{81}{section.7.1}\protected@file@percent }
\newlabel{FE2NN}{{7.1}{81}{Motivation: from finite element to neural network}{section.7.1}{}}
\newlabel{def_g}{{7.2}{81}{Motivation: from finite element to neural network}{equation.7.1.2}{}}
\newlabel{key}{{7.3}{81}{Motivation: from finite element to neural network}{equation.7.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Diagram of $\varphi (x)$ (left) and $\varphi _{\ell ,i}(x)$ (right).}}{82}{figure.7.1}\protected@file@percent }
\newlabel{key}{{7.4}{82}{Motivation: from finite element to neural network}{equation.7.1.4}{}}
\newlabel{key}{{7.5}{82}{Motivation: from finite element to neural network}{equation.7.1.5}{}}
\newlabel{key}{{7.6}{82}{Motivation: from finite element to neural network}{equation.7.1.6}{}}
\newlabel{key}{{7.7}{82}{Motivation: from finite element to neural network}{equation.7.1.7}{}}
\newlabel{key}{{7.8}{82}{Motivation: from finite element to neural network}{equation.7.1.8}{}}
\citation{he2020relu}
\newlabel{key}{{7.9}{83}{Motivation: from finite element to neural network}{equation.7.1.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Why we need deep neural networks via composition}{83}{section.7.2}\protected@file@percent }
\newlabel{whydeep}{{7.2}{83}{Why we need deep neural networks via composition}{section.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}FEM ans ${\rm  DNN}_1$ in 1D}{83}{subsection.7.2.1}\protected@file@percent }
\newlabel{key}{{7.10}{83}{FEM ans ${\rm DNN}_1$ in 1D}{equation.7.2.10}{}}
\newlabel{1d-basisu}{{7.11}{83}{FEM ans ${\rm DNN}_1$ in 1D}{equation.7.2.11}{}}
\newlabel{1d-basis}{{7.12}{83}{FEM ans ${\rm DNN}_1$ in 1D}{equation.7.2.12}{}}
\newlabel{thm:1dLFEMDNN}{{11}{83}{FEM ans ${\rm DNN}_1$ in 1D}{theorem.7.2.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Linear finite element cannot be recovered by ${\rm  DNN}_1$ for $d\ge 2$}{83}{subsection.7.2.2}\protected@file@percent }
\newlabel{key}{{7.13}{83}{Linear finite element cannot be recovered by ${\rm DNN}_1$ for $d\ge 2$}{equation.7.2.13}{}}
\newlabel{lowerbound}{{12}{84}{Linear finite element cannot be recovered by ${\rm DNN}_1$ for $d\ge 2$}{theorem.7.2.12}{}}
\newlabel{distcondi}{{7.14}{84}{Linear finite element cannot be recovered by ${\rm DNN}_1$ for $d\ge 2$}{equation.7.2.14}{}}
\newlabel{eq:disfun}{{7.15}{84}{Linear finite element cannot be recovered by ${\rm DNN}_1$ for $d\ge 2$}{equation.7.2.15}{}}
\newlabel{eq:dis_fn}{{7.16}{84}{Linear finite element cannot be recovered by ${\rm DNN}_1$ for $d\ge 2$}{equation.7.2.16}{}}
\newlabel{eq:def_gi}{{7.17}{84}{Linear finite element cannot be recovered by ${\rm DNN}_1$ for $d\ge 2$}{equation.7.2.17}{}}
\newlabel{eq: D_gi}{{7.18}{85}{Linear finite element cannot be recovered by ${\rm DNN}_1$ for $d\ge 2$}{equation.7.2.18}{}}
\newlabel{eq:assumD_gi}{{7.19}{85}{Linear finite element cannot be recovered by ${\rm DNN}_1$ for $d\ge 2$}{equation.7.2.19}{}}
\newlabel{eq:Dfcondition}{{7.20}{85}{Linear finite element cannot be recovered by ${\rm DNN}_1$ for $d\ge 2$}{equation.7.2.20}{}}
\newlabel{mergeH}{{7.2.2}{85}{Linear finite element cannot be recovered by ${\rm DNN}_1$ for $d\ge 2$}{equation.7.2.20}{}}
\citation{he2020relu}
\newlabel{linearindep}{{13}{86}{Linear finite element cannot be recovered by ${\rm DNN}_1$ for $d\ge 2$}{theorem.7.2.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Definition of deep neural networks (DNN)}{86}{section.7.3}\protected@file@percent }
\newlabel{thetamap1}{{7.21}{86}{Definition of deep neural networks (DNN)}{equation.7.3.21}{}}
\newlabel{sigma}{{7.22}{86}{Definition of deep neural networks (DNN)}{equation.7.3.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Definition of neurons}{86}{subsection.7.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Definition of deep neural network functions}{87}{subsection.7.3.2}\protected@file@percent }
\newlabel{sec:DNN}{{7.3.2}{87}{Definition of deep neural network functions}{subsection.7.3.2}{}}
\newlabel{compress-dnn}{{7.23}{87}{Definition of deep neural network functions}{equation.7.3.23}{}}
\citation{arora2016understanding}
\newlabel{NN1}{{7.24}{88}{Definition of deep neural network functions}{equation.7.3.24}{}}
\newlabel{NNL}{{7.25}{88}{Definition of deep neural network functions}{equation.7.3.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}ReLU DNN}{88}{subsection.7.3.3}\protected@file@percent }
\newlabel{relu}{{7.28}{88}{ReLU DNN}{equation.7.3.28}{}}
\newlabel{relu-dnn}{{7.29}{88}{ReLU DNN}{equation.7.3.29}{}}
\newlabel{dnn-cpwl}{{20}{88}{ReLU DNN}{lemma.20}{}}
\citation{strichartz2003guide}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Projections of the domain partitions formed by 2-layer ReLU DNNs with sizes $(n_0, n_1, n_2, n_3)= (2, 5, 5, 1), (2, 10, 10, 1) \text  {and}\ (2, 20, 20, 1)$ with random parameters.}}{89}{figure.7.2}\protected@file@percent }
\newlabel{fig:dnn-region}{{7.2}{89}{Projections of the domain partitions formed by 2-layer ReLU DNNs with sizes $(n_0, n_1, n_2, n_3)= (2, 5, 5, 1), (2, 10, 10, 1) \text {and}\ (2, 20, 20, 1)$ with random parameters}{figure.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.4}Fourier transform of polynomials}{89}{subsection.7.3.4}\protected@file@percent }
\newlabel{sigma-epsilon}{{7.31}{89}{Fourier transform of polynomials}{equation.7.3.31}{}}
\newlabel{eq_278}{{7.32}{89}{Fourier transform of polynomials}{equation.7.3.32}{}}
\newlabel{polynomial_lemma}{{21}{89}{Fourier transform of polynomials}{lemma.21}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Convolutional Multigrid Method}{91}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Two-point boundary problems and finite element discretization}{91}{section.8.1}\protected@file@percent }
\newlabel{minvar}{{8.1}{91}{Two-point boundary problems and finite element discretization}{equation.8.1.1}{}}
\newlabel{1Dpossion}{{8.2}{91}{Two-point boundary problems and finite element discretization}{equation.8.1.2}{}}
\newlabel{min1d}{{8.4}{92}{Two-point boundary problems and finite element discretization}{equation.8.1.4}{}}
\newlabel{1ddiscrevar}{{8.5}{92}{Two-point boundary problems and finite element discretization}{equation.8.1.5}{}}
\newlabel{thm:best-approximation}{{15}{92}{Two-point boundary problems and finite element discretization}{theorem.8.1.15}{}}
\newlabel{eq:best-approx}{{8.6}{92}{Two-point boundary problems and finite element discretization}{equation.8.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}Gradient descent method}{93}{subsection.8.1.1}\protected@file@percent }
\newlabel{min}{{8.8}{93}{Gradient descent method}{equation.8.1.8}{}}
\newlabel{1dRichardson}{{8.9}{94}{Gradient descent method}{equation.8.1.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces A picture on the GD method convergence history }}{95}{figure.8.1}\protected@file@percent }
\newlabel{fig:richardson}{{8.1}{95}{A picture on the GD method convergence history}{figure.8.1}{}}
\newlabel{jacobi1d}{{8.10}{95}{Gradient descent method}{equation.8.1.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces The smoothing effect of the gradient descent method }}{96}{figure.8.2}\protected@file@percent }
\newlabel{fig:smoothing}{{8.2}{96}{The smoothing effect of the gradient descent method}{figure.8.2}{}}
\newlabel{eq:convS}{{8.12}{96}{Gradient descent method}{equation.8.1.12}{}}
\newlabel{eq:kernel-S1d}{{8.13}{96}{Gradient descent method}{equation.8.1.13}{}}
\newlabel{eq:kernel-S2}{{8.14}{96}{Gradient descent method}{equation.8.1.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Convergence and smoothing properties of GD}{97}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fourier analysis for the gradient descent method}{97}{section*.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep =\parskip \topsep =\medskipamount \itemsep =\parskip \advance \itemsep by -\parsep {\leftmargin \leftmargini \parsep \z@ plus\p@ minus\p@ \topsep 6\p@ plus2\p@ minus4\p@ \itemsep \z@ }\belowdisplayskip \abovedisplayskip { $u-u^0$, $u-u^1$, $u-u^2$, $u-u^3$, $u-u^4$}}}{98}{figure.8.3}\protected@file@percent }
\newlabel{fig:Hmesh}{{8.3}{98}{\footnotesize { $u-u^0$, $u-u^1$, $u-u^2$, $u-u^3$, $u-u^4$}}{figure.8.3}{}}
\@writefile{toc}{\contentsline {paragraph}{An intuitive discussion}{98}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}Coarse grid correction and two grid method}{99}{subsection.8.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Two level grids.}}{99}{figure.8.4}\protected@file@percent }
\newlabel{Interpolation}{{8.4}{99}{Two level grids}{figure.8.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces Multiple grids in one dimension }}{99}{figure.8.5}\protected@file@percent }
\newlabel{fig:manygrids}{{8.5}{99}{Multiple grids in one dimension}{figure.8.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces A two grid method (in terms of FE functions)}}{100}{algorithm.7}\protected@file@percent }
\newlabel{alg:2grid:opezero}{{7}{100}{Coarse grid correction and two grid method}{algorithm.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Realization of step 1:}{101}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Realization of step 2:}{101}{section*.12}\protected@file@percent }
\newlabel{min2h}{{8.17}{101}{Realization of step 2:}{equation.8.1.17}{}}
\newlabel{prolongation}{{8.19}{101}{Realization of step 2:}{equation.8.1.19}{}}
\newlabel{rescon}{{8.20}{102}{Realization of step 2:}{equation.8.1.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{Realization of step 3:}{102}{section*.13}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}A two grid algorithm $\mu = {\text  {2G1}}(b; \mu ^0; 2,\nu _1, \nu _2)$}{103}{algorithm.8}\protected@file@percent }
\newlabel{alg:L-Slash11d}{{8}{103}{Realization of step 3:}{algorithm.8}{}}
\newlabel{eq:smoothing}{{8.24}{103}{Realization of step 3:}{equation.8.1.24}{}}
\newlabel{eq:smoothing}{{8.25}{103}{Realization of step 3:}{equation.8.1.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.3}Multilevel coarse grid corrections and a multigrid method}{103}{subsection.8.1.3}\protected@file@percent }
\newlabel{sc:mg-fd}{{8.1.3}{103}{Multilevel coarse grid corrections and a multigrid method}{subsection.8.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Multiple grids in one dimension }}{104}{figure.8.6}\protected@file@percent }
\newlabel{fig:manygrids}{{8.6}{104}{Multiple grids in one dimension}{figure.8.6}{}}
\newlabel{restrictk}{{8.26}{104}{Multilevel coarse grid corrections and a multigrid method}{equation.8.1.26}{}}
\newlabel{1drestriction}{{8.27}{104}{Multilevel coarse grid corrections and a multigrid method}{equation.8.1.27}{}}
\newlabel{1drestriction3}{{8.28}{104}{Multilevel coarse grid corrections and a multigrid method}{equation.8.1.28}{}}
\newlabel{prolongk}{{8.29}{104}{Multilevel coarse grid corrections and a multigrid method}{equation.8.1.29}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}A multigrid algorithm $\mu = {\text  {MG1}}(b; \mu ^0; J,\nu _1, \cdots  , \nu _J)$}{106}{algorithm.9}\protected@file@percent }
\newlabel{alg:L-Slash11dm}{{9}{106}{Multilevel coarse grid corrections and a multigrid method}{algorithm.9}{}}
\newlabel{eq:smoothing}{{8.31}{106}{Multilevel coarse grid corrections and a multigrid method}{equation.8.1.31}{}}
\newlabel{1Dposi}{{8.32}{106}{Multilevel coarse grid corrections and a multigrid method}{equation.8.1.32}{}}
\newlabel{matrix}{{8.33}{106}{Multilevel coarse grid corrections and a multigrid method}{equation.8.1.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces Comparison GD with Multigrid}}{107}{figure.8.7}\protected@file@percent }
\newlabel{fig:Hmesh}{{8.7}{107}{Comparison GD with Multigrid}{figure.8.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.4}Comparison between 1D and 2D finite element discretization}{108}{subsection.8.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Finite element method and convolution}{110}{section.8.2}\protected@file@percent }
\newlabel{sec:mg}{{8.2}{110}{Finite element method and convolution}{section.8.2}{}}
\newlabel{laplace}{{8.34}{110}{Finite element method and convolution}{equation.8.2.34}{}}
\newlabel{partitionyx}{{8.35}{110}{Finite element method and convolution}{equation.8.2.35}{}}
\newlabel{partitiony}{{8.36}{110}{Finite element method and convolution}{equation.8.2.36}{}}
\newlabel{fig:2dpartition}{{8.2}{110}{Finite element method and convolution}{equation.8.2.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces Two-dimensional uniform grids for finite element}}{110}{figure.8.8}\protected@file@percent }
\newlabel{Discrete:2d}{{8.37}{110}{Finite element method and convolution}{equation.8.2.37}{}}
\newlabel{NodalBasis}{{8.38}{110}{Finite element method and convolution}{equation.8.2.38}{}}
\newlabel{2d-fe0}{{8.40}{111}{Finite element method and convolution}{equation.8.2.40}{}}
\newlabel{fij_fe}{{8.41}{111}{Finite element method and convolution}{equation.8.2.41}{}}
\newlabel{def:convolution}{{11}{111}{Finite element method and convolution}{definition.11}{}}
\newlabel{con010}{{8.42}{111}{Finite element method and convolution}{equation.8.2.42}{}}
\newlabel{key}{{8.44}{111}{Finite element method and convolution}{equation.8.2.44}{}}
\newlabel{eq:padding}{{8.45}{111}{Finite element method and convolution}{equation.8.2.45}{}}
\newlabel{def:convolution2}{{12}{112}{Finite element method and convolution}{definition.12}{}}
\newlabel{stride_2}{{8.47}{112}{Finite element method and convolution}{equation.8.2.47}{}}
\newlabel{eq:Ac}{{8.48}{112}{Finite element method and convolution}{equation.8.2.48}{}}
\newlabel{Ac}{{8.49}{112}{Finite element method and convolution}{equation.8.2.49}{}}
\newlabel{prop:A}{{1}{112}{Finite element method and convolution}{proposition.1}{}}
\newlabel{minProblem}{{8.50}{112}{Finite element method and convolution}{equation.8.2.50}{}}
\citation{krizhevsky2012imagenet}
\newlabel{2d-fe1}{{8.52}{113}{Finite element method and convolution}{equation.8.2.52}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Piecewise linear functions on multilevel grids in 2D}{113}{section.8.3}\protected@file@percent }
\newlabel{sec:functions}{{8.3}{113}{Piecewise linear functions on multilevel grids in 2D}{section.8.3}{}}
\newlabel{grids}{{8.53}{113}{Piecewise linear functions on multilevel grids in 2D}{equation.8.3.53}{}}
\newlabel{mn-ell}{{8.54}{114}{Piecewise linear functions on multilevel grids in 2D}{equation.8.3.54}{}}
\newlabel{mgrid}{{8.3}{114}{Piecewise linear functions on multilevel grids in 2D}{equation.8.3.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces multilevel grids for piecewise linear functions}}{114}{figure.8.9}\protected@file@percent }
\newlabel{Vk}{{8.55}{114}{Piecewise linear functions on multilevel grids in 2D}{equation.8.3.55}{}}
\newlabel{mugrid-bi}{{8.3}{114}{Piecewise linear functions on multilevel grids in 2D}{equation.8.3.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces multilevel grids for piecewise bilinear functions}}{114}{figure.8.10}\protected@file@percent }
\newlabel{NodalBases-mul}{{8.56}{114}{Piecewise linear functions on multilevel grids in 2D}{equation.8.3.56}{}}
\newlabel{LinearNodalBasis}{{8.57}{115}{Piecewise linear functions on multilevel grids in 2D}{equation.8.3.57}{}}
\newlabel{BilinearNodalBasis}{{8.58}{115}{Piecewise linear functions on multilevel grids in 2D}{equation.8.3.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.11}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep =\parskip \topsep =\medskipamount \itemsep =\parskip \advance \itemsep by -\parsep {\leftmargin \leftmargini \parsep \z@ plus\p@ minus\p@ \topsep 6\p@ plus2\p@ minus4\p@ \itemsep \z@ }\belowdisplayskip \abovedisplayskip {Nodal basis for linear element.}}}{116}{figure.8.11}\protected@file@percent }
\newlabel{fig:nodallinear}{{8.11}{116}{\footnotesize {Nodal basis for linear element.}}{figure.8.11}{}}
\newlabel{dual-basis}{{8.59}{116}{Piecewise linear functions on multilevel grids in 2D}{equation.8.3.59}{}}
\newlabel{basis:plongation}{{8.60}{116}{Piecewise linear functions on multilevel grids in 2D}{equation.8.3.60}{}}
\newlabel{basis:plongation2}{{8.61}{116}{Piecewise linear functions on multilevel grids in 2D}{equation.8.3.61}{}}
\newlabel{expand}{{8.62}{116}{Piecewise linear functions on multilevel grids in 2D}{equation.8.3.62}{}}
\newlabel{vf}{{8.63}{117}{Piecewise linear functions on multilevel grids in 2D}{equation.8.3.63}{}}
\newlabel{v}{{8.64}{117}{Piecewise linear functions on multilevel grids in 2D}{equation.8.3.64}{}}
\newlabel{bi-restrict}{{8.65}{117}{Piecewise linear functions on multilevel grids in 2D}{equation.8.3.65}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Deconvolution}{117}{section.8.4}\protected@file@percent }
\newlabel{eq:def_deconv}{{8.66}{117}{Deconvolution}{equation.8.4.66}{}}
\newlabel{lemm:tilde-K}{{25}{117}{Deconvolution}{lemma.25}{}}
\newlabel{eq:}{{8.68}{117}{Deconvolution}{equation.8.4.68}{}}
\newlabel{eq:def_tildeK}{{8.69}{117}{Deconvolution}{equation.8.4.69}{}}
\newlabel{eq:op_deconv}{{8.70}{118}{Deconvolution}{equation.8.4.70}{}}
\newlabel{eq:de_stride_dim}{{8.71}{118}{Deconvolution}{equation.8.4.71}{}}
\newlabel{eq:de_stride}{{8.72}{118}{Deconvolution}{equation.8.4.72}{}}
\newlabel{eq:simple_deconv}{{8.73}{118}{Deconvolution}{equation.8.4.73}{}}
\newlabel{eq:final}{{8.74}{118}{Deconvolution}{equation.8.4.74}{}}
\newlabel{thm:deconv_op}{{17}{118}{Deconvolution}{theorem.8.4.17}{}}
\newlabel{eq:7}{{8.76}{118}{Deconvolution}{equation.8.4.76}{}}
\newlabel{eq:9}{{8.77}{118}{Deconvolution}{equation.8.4.77}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}Linear feature mappings}{119}{section.8.5}\protected@file@percent }
\newlabel{feature-map}{{8.79}{119}{Linear feature mappings}{equation.8.5.79}{}}
\newlabel{map-A}{{8.80}{119}{Linear feature mappings}{equation.8.5.80}{}}
\newlabel{map-A-ell}{{8.81}{119}{Linear feature mappings}{equation.8.5.81}{}}
\newlabel{feature-map-ell}{{8.82}{119}{Linear feature mappings}{equation.8.5.82}{}}
\newlabel{u_ell}{{8.83}{119}{Linear feature mappings}{equation.8.5.83}{}}
\newlabel{A-ell}{{8.84}{119}{Linear feature mappings}{equation.8.5.84}{}}
\newlabel{u-ell}{{8.85}{119}{Linear feature mappings}{equation.8.5.85}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.6}Restriction and prolongation under the convolution notation}{119}{section.8.6}\protected@file@percent }
\newlabel{restriction:freedom}{{8.87}{120}{Restriction and prolongation under the convolution notation}{equation.8.6.87}{}}
\newlabel{ris:plon}{{27}{120}{Restriction and prolongation under the convolution notation}{lemma.27}{}}
\newlabel{bi-restrict1}{{8.89}{120}{Restriction and prolongation under the convolution notation}{equation.8.6.89}{}}
\newlabel{eq:def_coarse}{{8.90}{121}{Restriction and prolongation under the convolution notation}{equation.8.6.90}{}}
\newlabel{eq:K=RAR}{{8.92}{121}{Restriction and prolongation under the convolution notation}{equation.8.6.92}{}}
\newlabel{eq:compute_A}{{8.93}{122}{Restriction and prolongation under the convolution notation}{equation.8.6.93}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.7}Multigrid for finite element methods}{122}{section.8.7}\protected@file@percent }
\newlabel{conA}{{8.96}{122}{Multigrid for finite element methods}{equation.8.7.96}{}}
\newlabel{fe0_Ka}{{8.97}{123}{Multigrid for finite element methods}{equation.8.7.97}{}}
\newlabel{iterativeAst}{{8.98}{123}{Multigrid for finite element methods}{equation.8.7.98}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.12}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep =\parskip \topsep =\medskipamount \itemsep =\parskip \advance \itemsep by -\parsep {\leftmargin \leftmargini \parsep \z@ plus\p@ minus\p@ \topsep 6\p@ plus2\p@ minus4\p@ \itemsep \z@ }\belowdisplayskip \abovedisplayskip {The errors of an random initial guess $u^0$, $u^{10}$, $u^{50}$ and $u^{100}$.}}}{124}{figure.8.12}\protected@file@percent }
\newlabel{fig:smooth}{{8.12}{124}{\footnotesize {The errors of an random initial guess $u^0$, $u^{10}$, $u^{50}$ and $u^{100}$.}}{figure.8.12}{}}
\newlabel{eta}{{8.99}{124}{Multigrid for finite element methods}{equation.8.7.99}{}}
\newlabel{jacobi1}{{8.100}{124}{Multigrid for finite element methods}{equation.8.7.100}{}}
\newlabel{eq:convS}{{8.102}{125}{Multigrid for finite element methods}{equation.8.7.102}{}}
\newlabel{eq:kernel-S}{{8.103}{125}{Multigrid for finite element methods}{equation.8.7.103}{}}
\newlabel{eq:kernel-S2}{{8.104}{125}{Multigrid for finite element methods}{equation.8.7.104}{}}
\newlabel{eta}{{8.105}{125}{Multigrid for finite element methods}{equation.8.7.105}{}}
\newlabel{jacobi1}{{8.106}{125}{Multigrid for finite element methods}{equation.8.7.106}{}}
\newlabel{eq:convS}{{8.108}{125}{Multigrid for finite element methods}{equation.8.7.108}{}}
\newlabel{eq:kernel-S}{{8.109}{126}{Multigrid for finite element methods}{equation.8.7.109}{}}
\newlabel{eq:kernel-S2}{{8.110}{126}{Multigrid for finite element methods}{equation.8.7.110}{}}
\newlabel{GD0}{{8.111}{126}{Multigrid for finite element methods}{equation.8.7.111}{}}
\newlabel{GD1}{{8.112}{126}{Multigrid for finite element methods}{equation.8.7.112}{}}
\newlabel{GD2}{{8.113}{126}{Multigrid for finite element methods}{equation.8.7.113}{}}
\newlabel{eq:smoothing0}{{8.114}{126}{Multigrid for finite element methods}{equation.8.7.114}{}}
\newlabel{eq:formresidual}{{8.115}{126}{Multigrid for finite element methods}{equation.8.7.115}{}}
\newlabel{erreq}{{8.116}{126}{Multigrid for finite element methods}{equation.8.7.116}{}}
\newlabel{coarse:correc}{{8.117}{126}{Multigrid for finite element methods}{equation.8.7.117}{}}
\newlabel{eq:prolongation00}{{8.118}{126}{Multigrid for finite element methods}{equation.8.7.118}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {10}$(u^{1}, u^2, \cdots  , u^J) = {\text  {MG0}}(f; u^0; J,\nu _1, \cdots  , \nu _J)$}{127}{algorithm.10}\protected@file@percent }
\newlabel{alg:L-Slash0}{{10}{127}{Multigrid for finite element methods}{algorithm.10}{}}
\newlabel{eq:smoothing}{{8.119}{127}{Multigrid for finite element methods}{equation.8.7.119}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {11}$u = {\text  {MG1}}(f; u^0; J,\nu _1, \cdots  , \nu _J)$}{127}{algorithm.11}\protected@file@percent }
\newlabel{alg:L-Slash1}{{11}{127}{Multigrid for finite element methods}{algorithm.11}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {12}$u = {\text  {MG2}}(f; u^0; J,\nu _1, \cdots  , \nu _J; \nu '_1, \cdots  , \nu '_J )$}{127}{algorithm.12}\protected@file@percent }
\newlabel{alg:V-cycle}{{12}{127}{Multigrid for finite element methods}{algorithm.12}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {13}$u = {\text  {multigrid1}}(f; u^0; J,\nu _1, \cdots  , \nu _J; \text  {tol})$;}{128}{algorithm.13}\protected@file@percent }
\newlabel{alg:multigrid-1}{{13}{128}{Multigrid for finite element methods}{algorithm.13}{}}
\newlabel{alg:multigrid-Pi}{{14}{128}{Multigrid for finite element methods}{algorithm.14}{}}
\newlabel{eq:smoothing}{{8.120}{129}{Multigrid for finite element methods}{equation.8.7.120}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.8}Numerical examples}{129}{section.8.8}\protected@file@percent }
\newlabel{laplacenu}{{8.121}{129}{Numerical examples}{equation.8.8.121}{}}
\newlabel{partitionyx}{{8.122}{129}{Numerical examples}{equation.8.8.122}{}}
\newlabel{partitiony}{{8.123}{129}{Numerical examples}{equation.8.8.123}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.9}ReLU multigrid method for nonnegative solution}{129}{section.8.9}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {15}$(u^{1}, u^2, \cdots  , u^J) = {\text  {MG0}}(f; u^0; J,\nu _1, \cdots  , \nu _J)$}{129}{algorithm.15}\protected@file@percent }
\newlabel{alg:L-ReLUSlash0}{{15}{129}{ReLU multigrid method for nonnegative solution}{algorithm.15}{}}
\newlabel{table:multivsGS}{{8.8}{130}{Numerical examples}{equation.8.8.123}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces Number of iterations for $\delimiter "026B30D  Ax - b \delimiter "026B30D / \delimiter "026B30D b\delimiter "026B30D  \leq 10^{-6}$.}}{130}{table.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.13}{\ignorespaces  Comparison GD with Multigrid.}}{130}{figure.8.13}\protected@file@percent }
\newlabel{fig:Hmesh}{{8.13}{130}{Comparison GD with Multigrid}{figure.8.13}{}}
\newlabel{eq:smoothing}{{8.124}{130}{ReLU multigrid method for nonnegative solution}{equation.8.9.124}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {16}$(u^{1}, u^2, \cdots  , u^J) = {\text  {MG0}}(f; u^0; J,\nu _1, \cdots  , \nu _J)$}{131}{algorithm.16}\protected@file@percent }
\newlabel{alg:L-ReLUSlash1}{{16}{131}{ReLU multigrid method for nonnegative solution}{algorithm.16}{}}
\newlabel{eq:smoothing}{{8.125}{131}{ReLU multigrid method for nonnegative solution}{equation.8.9.125}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9.1}$\Pi $ is interpolation}{131}{subsection.8.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.10}Multigrid methods for nonlinear problem}{132}{section.8.10}\protected@file@percent }
\newlabel{eq:rep}{{8.130}{132}{Multigrid methods for nonlinear problem}{equation.8.10.130}{}}
\newlabel{nonlinear:poisson}{{8.131}{132}{Multigrid methods for nonlinear problem}{equation.8.10.131}{}}
\newlabel{non:system}{{8.135}{132}{Multigrid methods for nonlinear problem}{equation.8.10.135}{}}
\newlabel{linearA}{{8.136}{132}{Multigrid methods for nonlinear problem}{equation.8.10.136}{}}
\newlabel{eq2}{{8.137}{133}{Multigrid methods for nonlinear problem}{equation.8.10.137}{}}
\newlabel{eq4}{{8.138}{133}{Multigrid methods for nonlinear problem}{equation.8.10.138}{}}
\newlabel{eq5}{{8.139}{133}{Multigrid methods for nonlinear problem}{equation.8.10.139}{}}
\citation{tai2002global,nash2000a}
\citation{briggs2000a,trottenberg2000multigrid}
\@writefile{loa}{\contentsline {algorithm}{\numberline {17}$u = {\text  { Bslash-FAS}}(u^{1,0},f,J,m_1, \cdots  , m_J)$}{134}{algorithm.17}\protected@file@percent }
\newlabel{alg:Slash-FAS}{{17}{134}{Multigrid methods for nonlinear problem}{algorithm.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.11}A nonlinear BVP example}{135}{section.8.11}\protected@file@percent }
\newlabel{eq:nolinear}{{8.141}{135}{A nonlinear BVP example}{equation.8.11.141}{}}
\newlabel{Table:nonliear}{{8.11}{136}{A nonlinear BVP example}{equation.8.11.141}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8.2}{\ignorespaces Number of iterations for $\delimiter "026B30D  L_h({\bm  {\alpha }}) \delimiter "026B30D / \delimiter "026B30D b\delimiter "026B30D  \leq 10^{-6}$}}{136}{table.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Convolutional Neural Networks}{137}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Nonlinear classifiable sets}{137}{section.9.1}\protected@file@percent }
\newlabel{key}{{9.1}{137}{Nonlinear classifiable sets}{equation.9.1.1}{}}
\newlabel{key}{{9.2}{137}{Nonlinear classifiable sets}{equation.9.1.2}{}}
\newlabel{key}{{9.3}{137}{Nonlinear classifiable sets}{equation.9.1.3}{}}
\newlabel{key}{{9.4}{137}{Nonlinear classifiable sets}{equation.9.1.4}{}}
\newlabel{key}{{9.5}{138}{Nonlinear classifiable sets}{equation.9.1.5}{}}
\newlabel{key}{{9.6}{138}{Nonlinear classifiable sets}{equation.9.1.6}{}}
\newlabel{key}{{9.7}{138}{Nonlinear classifiable sets}{equation.9.1.7}{}}
\newlabel{NonlinearBinary}{{9.8}{138}{Nonlinear classifiable sets}{equation.9.1.8}{}}
\newlabel{key}{{9.9}{138}{Nonlinear classifiable sets}{equation.9.1.9}{}}
\newlabel{key}{{9.10}{138}{Nonlinear classifiable sets}{equation.9.1.10}{}}
\newlabel{key}{{9.11}{138}{Nonlinear classifiable sets}{equation.9.1.11}{}}
\citation{krizhevsky2012imagenet}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Convolutional operations}{140}{section.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Images as matrix}{140}{subsection.9.2.1}\protected@file@percent }
\newlabel{sec:functions}{{9.2.1}{140}{Images as matrix}{subsection.9.2.1}{}}
\newlabel{grids}{{9.12}{140}{Images as matrix}{equation.9.2.12}{}}
\newlabel{mn-ell}{{9.13}{140}{Images as matrix}{equation.9.2.13}{}}
\newlabel{mugrid-bi}{{9.2.1}{141}{Images as matrix}{equation.9.2.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces multilevel grids for piecewise constant functions (images)}}{141}{figure.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}Convolution operation with one channel}{141}{subsection.9.2.2}\protected@file@percent }
\newlabel{eq:size_f}{{9.14}{141}{Convolution operation with one channel}{equation.9.2.14}{}}
\newlabel{con01}{{9.15}{141}{Convolution operation with one channel}{equation.9.2.15}{}}
\citation{bramble1977higher}
\citation{bramble1977higher}
\newlabel{key}{{9.17}{142}{Convolution operation with one channel}{equation.9.2.17}{}}
\newlabel{eq:padding}{{9.18}{142}{Convolution operation with one channel}{equation.9.2.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.3}Convolution with stride (one channel)}{142}{subsection.9.2.3}\protected@file@percent }
\newlabel{stride_2}{{9.20}{142}{Convolution with stride (one channel)}{equation.9.2.20}{}}
\newlabel{eq:convstride_2_1}{{9.21}{143}{Convolution with stride (one channel)}{equation.9.2.21}{}}
\newlabel{eq:strideopdim}{{9.22}{143}{Convolution with stride (one channel)}{equation.9.2.22}{}}
\newlabel{eq:strideop}{{9.23}{143}{Convolution with stride (one channel)}{equation.9.2.23}{}}
\newlabel{stride}{{9.25}{143}{Convolution with stride (one channel)}{equation.9.2.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.4}Convolutional operations with multi-channel}{144}{subsection.9.2.4}\protected@file@percent }
\newlabel{conv-1}{{9.26}{144}{Convolutional operations with multi-channel}{equation.9.2.26}{}}
\newlabel{con1}{{9.27}{144}{Convolutional operations with multi-channel}{equation.9.2.27}{}}
\newlabel{key}{{9.30}{144}{Convolutional operations with multi-channel}{equation.9.2.30}{}}
\newlabel{conv-1}{{9.31}{145}{Convolutional operations with multi-channel}{equation.9.2.31}{}}
\newlabel{key}{{9.32}{145}{Convolutional operations with multi-channel}{equation.9.2.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.5}Pooling operation in CNNs}{145}{subsection.9.2.5}\protected@file@percent }
\newlabel{key}{{9.33}{145}{Pooling operation in CNNs}{equation.9.2.33}{}}
\newlabel{key}{{9.34}{145}{Pooling operation in CNNs}{equation.9.2.34}{}}
\@writefile{toc}{\contentsline {paragraph}{Convolution with stride $s$ as pooling}{145}{section*.14}\protected@file@percent }
\newlabel{key}{{9.35}{145}{Convolution with stride $s$ as pooling}{equation.9.2.35}{}}
\@writefile{toc}{\contentsline {paragraph}{Nonlinear pooling}{145}{section*.15}\protected@file@percent }
\newlabel{key}{{9.36}{145}{Nonlinear pooling}{equation.9.2.36}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Examples of convolution filters and performance}{146}{section.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}Calculation with convolutions}{146}{subsection.9.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Image convolution examples}{146}{subsection.9.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}Line detection by 1D Laplacian}{146}{subsection.9.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Four convolutions to detect horizontal, vertical and lines at degrees 0,90,45,135.}}{147}{figure.9.2}\protected@file@percent }
\newlabel{line:detect1}{{9.2}{147}{Four convolutions to detect horizontal, vertical and lines at degrees 0,90,45,135}{figure.9.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces \nobreakspace  {}\nobreakspace  {}A horizontal line detection done with convolutions.}}{147}{figure.9.3}\protected@file@percent }
\newlabel{line:detect2}{{9.3}{147}{~~A horizontal line detection done with convolutions}{figure.9.3}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {image}}}{147}{figure.9.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {filter}}}{147}{figure.9.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {result}}}{147}{figure.9.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces \nobreakspace  {}\nobreakspace  {}A vertical line detection done with convolutions.}}{147}{figure.9.4}\protected@file@percent }
\newlabel{line:detect4}{{9.4}{147}{~~A vertical line detection done with convolutions}{figure.9.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces \nobreakspace  {}\nobreakspace  {}A 45 degree line detection done with convolutions.}}{148}{figure.9.5}\protected@file@percent }
\newlabel{line:detect6}{{9.5}{148}{~~A 45 degree line detection done with convolutions}{figure.9.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.6}{\ignorespaces \nobreakspace  {}\nobreakspace  {}A 135 degree line detection done with convolutions.}}{148}{figure.9.6}\protected@file@percent }
\newlabel{line:detect8}{{9.6}{148}{~~A 135 degree line detection done with convolutions}{figure.9.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.4}Edge detection by 2D Laplacian operator}{148}{subsection.9.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {image}}}{148}{subsection.9.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {filter}}}{148}{subsection.9.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {result}}}{148}{subsection.9.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {image}}}{148}{subsection.9.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {filter}}}{148}{subsection.9.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {result}}}{148}{subsection.9.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {image}}}{148}{subsection.9.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {filter}}}{148}{subsection.9.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {result}}}{148}{subsection.9.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.7}{\ignorespaces \nobreakspace  {}\nobreakspace  {}A laplace operator done with convolutions}}{149}{figure.9.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.8}{\ignorespaces \nobreakspace  {}\nobreakspace  {}A laplace operator including diagonals are done with convolutions}}{150}{figure.9.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.5}The Laplacian of Gaussian}{150}{subsection.9.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {subtable}{\numberline{(a)}{\ignorespaces {image}}}{150}{subsection.9.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {subtable}{\numberline{(b)}{\ignorespaces {filter}}}{150}{subsection.9.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {subtable}{\numberline{(c)}{\ignorespaces {result}}}{150}{subsection.9.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {subtable}{\numberline{(d)}{\ignorespaces {image}}}{150}{subsection.9.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {subtable}{\numberline{(e)}{\ignorespaces {result}}}{150}{subsection.9.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {subtable}{\numberline{(a)}{\ignorespaces {image}}}{150}{subsection.9.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {subtable}{\numberline{(b)}{\ignorespaces {filter}}}{150}{subsection.9.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {subtable}{\numberline{(c)}{\ignorespaces {result}}}{150}{subsection.9.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {subtable}{\numberline{(d)}{\ignorespaces {image}}}{150}{subsection.9.3.5}\protected@file@percent }
\@writefile{lot}{\contentsline {subtable}{\numberline{(e)}{\ignorespaces {result}}}{150}{subsection.9.3.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.9}{\ignorespaces \nobreakspace  {}\nobreakspace  {}A Laplacian of Gaussian operator done with convolutions}}{151}{figure.9.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.6}Other examples with ReLU activation}{151}{subsection.9.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.7}Summary}{151}{subsection.9.3.7}\protected@file@percent }
\citation{lecun1998gradient}
\citation{krizhevsky2012imagenet}
\citation{simonyan2014very}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Some popular CNN models}{152}{section.9.4}\protected@file@percent }
\newlabel{sec:CNNs}{{9.4}{152}{Some popular CNN models}{section.9.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.1}LeNet-5, AlexNet and VGG}{152}{subsection.9.4.1}\protected@file@percent }
\citation{he2016deep}
\@writefile{loa}{\contentsline {algorithm}{\numberline {18}$ h = \text  {Classic CNN}(f; J,\nu _1, \cdots  , \nu _J)$}{153}{algorithm.18}\protected@file@percent }
\newlabel{alg:presnet}{{18}{153}{LeNet-5, AlexNet and VGG}{algorithm.18}{}}
\newlabel{ori-ResNet}{{9.39}{153}{LeNet-5, AlexNet and VGG}{equation.9.4.39}{}}
\newlabel{ori-ResNet0}{{9.40}{153}{LeNet-5, AlexNet and VGG}{equation.9.4.40}{}}
\newlabel{key}{{9.41}{153}{LeNet-5, AlexNet and VGG}{equation.9.4.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.2}ResNet}{153}{subsection.9.4.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {19}$ h = \text  {ResNet}(f; J,\nu _1, \cdots  , \nu _J)$}{153}{algorithm.19}\protected@file@percent }
\newlabel{alg:resnet}{{19}{153}{ResNet}{algorithm.19}{}}
\citation{krizhevsky2009learning}
\citation{deng2009imagenet}
\citation{he2016identity}
\citation{he2016identity}
\citation{he2016identity}
\newlabel{ori-ResNet}{{9.42}{154}{ResNet}{equation.9.4.42}{}}
\newlabel{ori-ResNet0}{{9.43}{154}{ResNet}{equation.9.4.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.3}pre-act ResNet}{154}{subsection.9.4.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {20}$ h = \text  {pre-act ResNet}(f; J,\nu _1, \cdots  , \nu _J)$}{154}{algorithm.20}\protected@file@percent }
\newlabel{alg:presnet}{{20}{154}{pre-act ResNet}{algorithm.20}{}}
\newlabel{ori-ResNet}{{9.44}{154}{pre-act ResNet}{equation.9.4.44}{}}
\newlabel{ori-ResNet0}{{9.45}{154}{pre-act ResNet}{equation.9.4.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.10}{\ignorespaces Comparison of CNN Structures}}{155}{figure.9.10}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {image}}}{155}{figure.9.10}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {filter}}}{155}{figure.9.10}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {result}}}{155}{figure.9.10}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {image}}}{155}{figure.9.10}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {result}}}{155}{figure.9.10}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {input image}}}{155}{figure.9.10}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {filter}}}{155}{figure.9.10}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {convolution result}}}{155}{figure.9.10}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {result after ReLU}}}{155}{figure.9.10}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {filter}}}{155}{figure.9.10}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {result after average}}}{155}{figure.9.10}\protected@file@percent }
\newlabel{eq:cCNN}{{9.46}{155}{pre-act ResNet}{equation.9.4.46}{}}
\newlabel{eq:ResNet}{{9.47}{155}{pre-act ResNet}{equation.9.4.47}{}}
\newlabel{eq:pre-act ResNet}{{9.48}{155}{pre-act ResNet}{equation.9.4.48}{}}
\citation{he2019mgnet}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}MgNet: a Unified Framework for CNN and MG}{157}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}MgNet: a new network structure}{157}{section.10.1}\protected@file@percent }
\newlabel{sec:mgnet}{{10.1}{157}{MgNet: a new network structure}{section.10.1}{}}
\newlabel{eq:iterscheme}{{10.1}{157}{MgNet: a new network structure}{equation.10.1.1}{}}
\newlabel{eq:inter&rest}{{10.2}{157}{MgNet: a new network structure}{equation.10.1.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {21}$u^{J}={\rm  MgNet}(f; J,\nu _1, \cdots  , \nu _J)$}{157}{algorithm.21}\protected@file@percent }
\newlabel{alg:mgnet}{{21}{157}{MgNet: a new network structure}{algorithm.21}{}}
\newlabel{mgnet}{{10.3}{157}{MgNet: a new network structure}{equation.10.1.3}{}}
\newlabel{interpolation}{{10.4}{158}{MgNet: a new network structure}{equation.10.1.4}{}}
\newlabel{restrict-f}{{10.5}{158}{MgNet: a new network structure}{equation.10.1.5}{}}
\newlabel{thm:mg-mgnet}{{21}{158}{MgNet: a new network structure}{theorem.10.1.21}{}}
\newlabel{eq:f-u}{{10.6}{158}{MgNet: a new network structure}{equation.10.1.6}{}}
\newlabel{u:i+1}{{10.7}{158}{MgNet: a new network structure}{equation.10.1.7}{}}
\citation{he2019constrained}
\newlabel{Auf-ell}{{10.8}{159}{MgNet: a new network structure}{equation.10.1.8}{}}
\newlabel{f-ell}{{10.9}{159}{MgNet: a new network structure}{equation.10.1.9}{}}
\newlabel{u-ell}{{10.10}{159}{MgNet: a new network structure}{equation.10.1.10}{}}
\newlabel{key}{{10.11}{159}{MgNet: a new network structure}{equation.10.1.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces Structure of MgNet}}{159}{figure.10.1}\protected@file@percent }
\newlabel{fig:mgnet}{{10.1}{159}{Structure of MgNet}{figure.10.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.1}Initialization: feature space channels}{160}{subsection.10.1.1}\protected@file@percent }
\newlabel{eq:6}{{10.12}{160}{Initialization: feature space channels}{equation.10.1.12}{}}
\newlabel{cc}{{10.13}{160}{Initialization: feature space channels}{equation.10.1.13}{}}
\newlabel{decomp-f}{{10.14}{160}{Initialization: feature space channels}{equation.10.1.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.2}Extracted units: $u^{\ell }$ and channels}{160}{subsection.10.1.2}\protected@file@percent }
\newlabel{uf-channels}{{10.15}{160}{Extracted units: $u^{\ell }$ and channels}{equation.10.1.15}{}}
\newlabel{cfl}{{10.16}{160}{Extracted units: $u^{\ell }$ and channels}{equation.10.1.16}{}}
\newlabel{ufl}{{10.17}{160}{Extracted units: $u^{\ell }$ and channels}{equation.10.1.17}{}}
\citation{he2016identity}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.3}Poolings: $\Pi _{\ell +1}^\ell $ and $R_{\ell +1}^\ell $}{161}{subsection.10.1.3}\protected@file@percent }
\newlabel{eq:8}{{10.18}{161}{Poolings: $\Pi _{\ell +1}^\ell $ and $R_{\ell +1}^\ell $}{equation.10.1.18}{}}
\newlabel{Pi}{{10.19}{161}{Poolings: $\Pi _{\ell +1}^\ell $ and $R_{\ell +1}^\ell $}{equation.10.1.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.4}Data-feature mapping: $A^{\ell }$}{161}{subsection.10.1.4}\protected@file@percent }
\newlabel{u-resnet}{{10.20}{161}{Data-feature mapping: $A^{\ell }$}{equation.10.1.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.5}Feature extractors: $\sigma \circ B^{\ell ,i} \ast \sigma $}{161}{subsection.10.1.5}\protected@file@percent }
\newlabel{extractor-ell}{{10.21}{161}{Feature extractors: $\sigma \circ B^{\ell ,i} \ast \sigma $}{equation.10.1.21}{}}
\citation{he2016identity}
\citation{xu2017algebraic}
\citation{he2016deep}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}MgNet, pre-act ResNet, variants and generalizations}{162}{section.10.2}\protected@file@percent }
\newlabel{sec:relation}{{10.2}{162}{MgNet, pre-act ResNet, variants and generalizations}{section.10.2}{}}
\newlabel{thm:mgnet1}{{22}{162}{MgNet, pre-act ResNet, variants and generalizations}{theorem.10.2.22}{}}
\newlabel{dualmgnet}{{10.22}{162}{MgNet, pre-act ResNet, variants and generalizations}{equation.10.2.22}{}}
\newlabel{eq:5}{{10.23}{162}{MgNet, pre-act ResNet, variants and generalizations}{equation.10.2.23}{}}
\newlabel{thm:mgnet2}{{31}{162}{MgNet, pre-act ResNet, variants and generalizations}{lemma.31}{}}
\newlabel{tilde-resnet}{{10.24}{162}{MgNet, pre-act ResNet, variants and generalizations}{equation.10.2.24}{}}
\newlabel{tilde-f}{{10.25}{162}{MgNet, pre-act ResNet, variants and generalizations}{equation.10.2.25}{}}
\newlabel{resnet1}{{10.26}{162}{MgNet, pre-act ResNet, variants and generalizations}{equation.10.2.26}{}}
\newlabel{mg-resnet}{{10.27}{162}{MgNet, pre-act ResNet, variants and generalizations}{equation.10.2.27}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10.1}{\ignorespaces Comparison for all iterative forms }}{163}{table.10.1}\protected@file@percent }
\newlabel{comparison-ALL}{{10.1}{163}{Comparison for all iterative forms}{table.10.1}{}}
\newlabel{fig:mgnet}{{10.2}{163}{MgNet, pre-act ResNet, variants and generalizations}{table.10.1}{}}
\newlabel{thm:CNN}{{23}{163}{MgNet, pre-act ResNet, variants and generalizations}{theorem.10.2.23}{}}
\newlabel{CNN1}{{10.28}{164}{MgNet, pre-act ResNet, variants and generalizations}{equation.10.2.28}{}}
\newlabel{Res-CNN1}{{10.29}{164}{MgNet, pre-act ResNet, variants and generalizations}{equation.10.2.29}{}}
\newlabel{CNN2}{{10.30}{164}{MgNet, pre-act ResNet, variants and generalizations}{equation.10.2.30}{}}
\newlabel{Res-CNN2}{{10.31}{164}{MgNet, pre-act ResNet, variants and generalizations}{equation.10.2.31}{}}
\newlabel{xi-cnn1}{{10.32}{164}{MgNet, pre-act ResNet, variants and generalizations}{equation.10.2.32}{}}
\newlabel{eta-ell}{{10.33}{164}{MgNet, pre-act ResNet, variants and generalizations}{equation.10.2.33}{}}
\newlabel{eq:hatdelta}{{10.36}{164}{MgNet, pre-act ResNet, variants and generalizations}{equation.10.2.36}{}}
\citation{he2019constrained}
\citation{xu2017algebraic}
\citation{he2019mgnet}
\@writefile{toc}{\contentsline {section}{\numberline {10.3}Constrained linear data-feature mapping from MgNet to interpret ResNet}{165}{section.10.3}\protected@file@percent }
\citation{xu1992iterative,hackbusch2013multi,xu2017algebraic}
\citation{xu1992iterative}
\citation{xu1992iterative,hackbusch1994iterative,golub2012matrix}
\newlabel{eq:fmapping}{{10.50}{166}{Constrained linear data-feature mapping from MgNet to interpret ResNet}{equation.10.3.50}{}}
\newlabel{eq:positive}{{10.51}{166}{Constrained linear data-feature mapping from MgNet to interpret ResNet}{equation.10.3.51}{}}
\newlabel{BAmapping}{{10.52}{166}{Constrained linear data-feature mapping from MgNet to interpret ResNet}{equation.10.3.52}{}}
\newlabel{eq:uBfAu}{{10.53}{166}{Constrained linear data-feature mapping from MgNet to interpret ResNet}{equation.10.3.53}{}}
\newlabel{eq:pre-actResNet_residual}{{10.54}{166}{Constrained linear data-feature mapping from MgNet to interpret ResNet}{equation.10.3.54}{}}
\newlabel{thm:1}{{24}{166}{Constrained linear data-feature mapping from MgNet to interpret ResNet}{theorem.10.3.24}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Initializations and Normalizations}{167}{chapter.11}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}Data normalization in DNNs and CNNs}{167}{section.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.1}Data normalization in DNNs}{167}{subsection.11.1.1}\protected@file@percent }
\newlabel{eq:trainingdata}{{11.1}{167}{Data normalization in DNNs}{equation.11.1.1}{}}
\newlabel{key}{{11.2}{167}{Data normalization in DNNs}{equation.11.1.2}{}}
\newlabel{key}{{11.3}{167}{Data normalization in DNNs}{equation.11.1.3}{}}
\newlabel{eq:normlizationData}{{11.4}{168}{Data normalization in DNNs}{equation.11.1.4}{}}
\newlabel{key}{{11.5}{168}{Data normalization in DNNs}{equation.11.1.5}{}}
\newlabel{key}{{11.6}{168}{Data normalization in DNNs}{equation.11.1.6}{}}
\newlabel{key}{{11.7}{168}{Data normalization in DNNs}{equation.11.1.7}{}}
\newlabel{key}{{11.8}{168}{Data normalization in DNNs}{equation.11.1.8}{}}
\newlabel{key}{{11.9}{168}{Data normalization in DNNs}{equation.11.1.9}{}}
\newlabel{key}{{11.10}{168}{Data normalization in DNNs}{equation.11.1.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.2}Data normalization for images in CNNs}{168}{subsection.11.1.2}\protected@file@percent }
\newlabel{key}{{11.11}{168}{Data normalization for images in CNNs}{equation.11.1.11}{}}
\newlabel{key}{{11.12}{169}{Data normalization for images in CNNs}{equation.11.1.12}{}}
\newlabel{key}{{11.13}{169}{Data normalization for images in CNNs}{equation.11.1.13}{}}
\newlabel{key}{{11.14}{169}{Data normalization for images in CNNs}{equation.11.1.14}{}}
\newlabel{key}{{11.15}{169}{Data normalization for images in CNNs}{equation.11.1.15}{}}
\newlabel{key}{{11.16}{169}{Data normalization for images in CNNs}{equation.11.1.16}{}}
\citation{glorot2010understanding}
\@writefile{toc}{\contentsline {section}{\numberline {11.2}Initialization for deep neural networks}{170}{section.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.1}Xavier's Initialization with $\sigma = id$}{170}{subsection.11.2.1}\protected@file@percent }
\newlabel{key}{{11.19}{170}{Xavier's Initialization with $\sigma = id$}{equation.11.2.19}{}}
\newlabel{lemm:init}{{32}{170}{Xavier's Initialization with $\sigma = id$}{lemma.32}{}}
\newlabel{eq:FWini}{{11.20}{170}{Xavier's Initialization with $\sigma = id$}{equation.11.2.20}{}}
\newlabel{eq:FWini}{{11.24}{171}{Xavier's Initialization with $\sigma = id$}{equation.11.2.24}{}}
\newlabel{th:idnormal}{{33}{171}{Xavier's Initialization with $\sigma = id$}{lemma.33}{}}
\newlabel{key}{{11.25}{171}{Xavier's Initialization with $\sigma = id$}{equation.11.2.25}{}}
\newlabel{key}{{11.26}{171}{Xavier's Initialization with $\sigma = id$}{equation.11.2.26}{}}
\newlabel{key}{{11.27}{171}{Xavier's Initialization with $\sigma = id$}{equation.11.2.27}{}}
\newlabel{key}{{11.28}{171}{Xavier's Initialization with $\sigma = id$}{equation.11.2.28}{}}
\newlabel{key}{{11.29}{172}{Xavier's Initialization with $\sigma = id$}{equation.11.2.29}{}}
\newlabel{varianceW}{{11.30}{172}{Xavier's Initialization with $\sigma = id$}{equation.11.2.30}{}}
\newlabel{key}{{11.32}{172}{Xavier's Initialization with $\sigma = id$}{equation.11.2.32}{}}
\newlabel{key}{{11.33}{172}{Xavier's Initialization with $\sigma = id$}{equation.11.2.33}{}}
\newlabel{key}{{11.34}{172}{Xavier's Initialization with $\sigma = id$}{equation.11.2.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.2}Variance analysis in backward propagation phase}{173}{subsection.11.2.2}\protected@file@percent }
\newlabel{th:idnormal2}{{34}{173}{Variance analysis in backward propagation phase}{lemma.34}{}}
\newlabel{normale0}{{11.36}{173}{Variance analysis in backward propagation phase}{equation.11.2.36}{}}
\newlabel{normale1}{{11.37}{174}{Variance analysis in backward propagation phase}{equation.11.2.37}{}}
\newlabel{normale2}{{11.38}{174}{Variance analysis in backward propagation phase}{equation.11.2.38}{}}
\citation{he2015delving}
\@writefile{toc}{\contentsline {paragraph}{Question}{175}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.3}Kaiming's initialization}{175}{subsection.11.2.3}\protected@file@percent }
\newlabel{key}{{11.39}{175}{Kaiming's initialization}{equation.11.2.39}{}}
\newlabel{key}{{11.40}{176}{Kaiming's initialization}{equation.11.2.40}{}}
\newlabel{key}{{11.41}{176}{Kaiming's initialization}{equation.11.2.41}{}}
\newlabel{key}{{11.42}{176}{Kaiming's initialization}{equation.11.2.42}{}}
\newlabel{key}{{11.43}{176}{Kaiming's initialization}{equation.11.2.43}{}}
\newlabel{key}{{11.44}{176}{Kaiming's initialization}{equation.11.2.44}{}}
\newlabel{key}{{11.45}{176}{Kaiming's initialization}{equation.11.2.45}{}}
\citation{he2015delving}
\citation{he2015delving}
\citation{he2015delving}
\newlabel{key}{{11.2.3}{177}{Kaiming's initialization}{equation.11.2.45}{}}
\newlabel{key}{{11.46}{177}{Kaiming's initialization}{equation.11.2.46}{}}
\newlabel{key}{{11.47}{177}{Kaiming's initialization}{equation.11.2.47}{}}
\newlabel{key}{{11.48}{177}{Kaiming's initialization}{equation.11.2.48}{}}
\newlabel{key}{{11.49}{177}{Kaiming's initialization}{equation.11.2.49}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.3}Data normalization in CNNs}{177}{section.11.3}\protected@file@percent }
\newlabel{key}{{11.51}{177}{Data normalization in CNNs}{equation.11.3.51}{}}
\newlabel{key}{{11.52}{177}{Data normalization in CNNs}{equation.11.3.52}{}}
\newlabel{key}{{11.53}{177}{Data normalization in CNNs}{equation.11.3.53}{}}
\citation{glorot2010understanding}
\citation{glorot2010understanding}
\citation{glorot2010understanding}
\citation{glorot2010understanding}
\newlabel{key}{{11.54}{178}{Data normalization in CNNs}{equation.11.3.54}{}}
\newlabel{key}{{11.55}{178}{Data normalization in CNNs}{equation.11.3.55}{}}
\newlabel{key}{{11.56}{178}{Data normalization in CNNs}{equation.11.3.56}{}}
\newlabel{key}{{11.57}{178}{Data normalization in CNNs}{equation.11.3.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.1}{\ignorespaces The convergence of a \textbf  {22-layer} large model. The $x$-axis is the number of training epochs. The y-axis is the top-1 error of 3,000 random val samples, evaluated on the center crop. Use ReLU as the activation for both cases. Both Kaiming's initialization (red) and ``\emph  {Xavier's}'' (blue) \cite  {glorot2010understanding} lead to convergence, but Kaiming's initialization starts reducing error earlier.}}{178}{figure.11.1}\protected@file@percent }
\newlabel{fig:converge_22layers}{{11.1}{178}{The convergence of a \textbf {22-layer} large model. The $x$-axis is the number of training epochs. The y-axis is the top-1 error of 3,000 random val samples, evaluated on the center crop. Use ReLU as the activation for both cases. Both Kaiming's initialization (red) and ``\emph {Xavier's}'' (blue) \cite {glorot2010understanding} lead to convergence, but Kaiming's initialization starts reducing error earlier}{figure.11.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.2}{\ignorespaces The convergence of a \textbf  {30-layer} small model (see the main text). Use ReLU as the activation for both cases. Kaiming's initialization (red) is able to make it converge. But ``\emph  {Xavier's}'' (blue) \cite  {glorot2010understanding} completely stalls - It is also verified that that its gradients are all diminishing. It does not converge even given more epochs.}}{179}{figure.11.2}\protected@file@percent }
\newlabel{fig:converge_30layers}{{11.2}{179}{The convergence of a \textbf {30-layer} small model (see the main text). Use ReLU as the activation for both cases. Kaiming's initialization (red) is able to make it converge. But ``\emph {Xavier's}'' (blue) \cite {glorot2010understanding} completely stalls - It is also verified that that its gradients are all diminishing. It does not converge even given more epochs}{figure.11.2}{}}
\citation{ioffe2015batch}
\@writefile{toc}{\contentsline {section}{\numberline {11.4}Batch Normalization in DNN and CNN}{180}{section.11.4}\protected@file@percent }
\newlabel{key}{{11.58}{180}{Batch Normalization in DNN and CNN}{equation.11.4.58}{}}
\newlabel{eq:DNNdef_J}{{11.59}{180}{Batch Normalization in DNN and CNN}{equation.11.4.59}{}}
\newlabel{DL-process}{{11.60}{180}{Batch Normalization in DNN and CNN}{equation.11.4.60}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.1}Ideas Behind the BN for DNN: Internal \unhbox \voidb@x \hbox {Covariate} Shift in Training}{180}{subsection.11.4.1}\protected@file@percent }
\citation{lecun1998neural,wiesler2011convergence}
\citation{lecun1998neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.2}Practical batch normalization: assume i.i.d and add scale and shift }{181}{subsection.11.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Take batch normalization for each scalar feature (neuron).}{181}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Add scale and shift into batch normalization.}{182}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.3}Batch normalization for DNN}{182}{subsection.11.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition of batch normalization operation based on the batch}{182}{section*.19}\protected@file@percent }
\newlabel{eq:trainingdata}{{11.64}{182}{Definition of batch normalization operation based on the batch}{equation.11.4.64}{}}
\newlabel{def:BNeq}{{11.65}{182}{Definition of batch normalization operation based on the batch}{equation.11.4.65}{}}
\newlabel{eq:BNop}{{11.66}{183}{Definition of batch normalization operation based on the batch}{equation.11.4.66}{}}
\@writefile{toc}{\contentsline {paragraph}{Model with batch normalization}{183}{section*.20}\protected@file@percent }
\newlabel{nn-BN0}{{11.67}{183}{Model with batch normalization}{equation.11.4.67}{}}
\newlabel{nn-BN}{{11.69}{183}{Model with batch normalization}{equation.11.4.69}{}}
\newlabel{eq:loss-BN}{{11.71}{183}{Model with batch normalization}{equation.11.4.71}{}}
\newlabel{eq:threeExpectation}{{11.72}{183}{Model with batch normalization}{equation.11.4.72}{}}
\citation{ioffe2015batch}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.4}Batch normalization: some ``modified" SGD training algorithm}{184}{subsection.11.4.4}\protected@file@percent }
\newlabel{eq:mini-batch-sample}{{11.73}{184}{Batch normalization: some ``modified" SGD training algorithm}{equation.11.4.73}{}}
\newlabel{nn-BN-training}{{11.74}{184}{Batch normalization: some ``modified" SGD training algorithm}{equation.11.4.74}{}}
\newlabel{def:BNeq-traning}{{11.76}{184}{Batch normalization: some ``modified" SGD training algorithm}{equation.11.4.76}{}}
\citation{ioffe2015batch}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.5}Final model with BN in DNN after training}{185}{subsection.11.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.6}Batch Normalization for CNN}{185}{subsection.11.4.6}\protected@file@percent }
\bibstyle{abbrv}
\bibdata{ref,ref_mgcnn,../3FEM/3FEM}
\newlabel{def:BNeq-traningCNN}{{11.80}{186}{Batch Normalization for CNN}{equation.11.4.80}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.7}Batch normalization for MgNet}{186}{subsection.11.4.7}\protected@file@percent }
\newlabel{key}{{11.81}{186}{Batch normalization for MgNet}{equation.11.4.81}{}}
\newlabel{key}{{11.82}{186}{Batch normalization for MgNet}{equation.11.4.82}{}}
\newlabel{key}{{11.83}{186}{Batch normalization for MgNet}{equation.11.4.83}{}}
