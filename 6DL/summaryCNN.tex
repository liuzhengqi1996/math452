\subsection{Summary}

\subsubsection{Data augmentation}
\begin{enumerate}
\item The mathmatical detail of fast autoaugment.
\end{enumerate}

\begin{enumerate}
\item Applying fast auto-augmentation on the test set would significantly decrease the test accuracy given a saved model by about 4\%.
\item Applying the basic augmentation (RandomCrop and RandomHorizontalFlip) on the test set does not affect the test accuracy given a saved model. 
\item Fast auto-augmentation is a extremely powerful augmentation technique. It can wildly change the original picture in hundreds of ways and thus achieve very good generalization ability.
\end{enumerate}
\subsubsection{Cross validation}
\begin{enumerate}
\item People use the test set multiple times. The best they can do is to run the same setup of the experiment multiple times to ensure the robustness and repeatability. With our cross-validation framework, we can fairly compare the performance of the proposed model and training algorithms.
\item The regular cross validation taking validation set and test set sequentially achieved better accuracy for all the experiments than the overlapping cross validation. We also prefer this way since we don't need to consider the problem of repeated data in different folds for the cross validation taking random validation set and test set.
\item The regular cross validation taking validation set and test set sequentially achieved comparable  accuracy with the leave-one-fold cross validation. It suggests that when the test data is already given, there is not much difference about the data splitting and model selection.
\item In the overlapping $k$-fold cross validation, the total number of outliers in all the test folds follows a binomial distribution and can be quite different. Therefore we don't want to use this method for evaluation.
\item The average test accuracy is comparable as for taking the model with best validation accuracy or the final model after training. This coincides with our observation that these two ways don't make a difference. It also explains why both ways exist when researchers are reporting their results.
\end{enumerate}
\subsubsection{MgNet related}
\begin{enumerate}
	\item only use activation in the first grid can get some good validation accuracy
	\item increasing all $c^\ell$ together always help, showed in table \ref{data setting3  channel 128}
	\item increasing $\nu_1$ always help when $\nu_1$ is not too big, showed in table \ref{data setting3  channel 128} and table \ref{different ite num in 2nd grid, not fix ell}
	\item when $\nu_\ell=0$ increasing $\nu_\ell, \ell>1$  always help, showed in table \ref{different ite num in 2nd grid} and table \ref{different ite num in 2nd grid, not fix ell-case0}
	
	\item when $\nu_\ell \geq 2$, increasing $\nu_\ell, \ell>1$ help model prevent over fitting and get higher training accuracy, but it's not help for validation, showed in table \ref{different ite num in 2nd grid case1}
	\item with small $c^1$, increasing $c^\ell, \ell=1$ is just good for training accuracy but not benifit for validation accuracy, showed in table \ref{different ite num in 2nd grid, not fix ell}
	\item sharing B increase validation accuracy in raw MgNet, showed in \ref{raw MgNet results}
	\item sharing B increase training accuracynot in MgNet remove some activation but not help for validation , show in \ref{data setting3  share B} 	
	\item BN without relu help training accuracy, and it can work with bigger lr which makes computation overflow without BN
	\item BN without relu enlarge over-fitting
	\item use same kernel in every grid, it reduces the parameters to a quarter and still remains some good validation accuracy
\end{enumerate}
\subsubsection{Random parameters and random labels}
\begin{enumerate}
	\item On Cifar100, the training accuracy (on training+validation datasets) is $99.9600\%$ if we randomly generate the parameters in the input layer (8 channels)
	\item On Cifar100, the training accuracy (on training+validation datasets) is $94\%$ if we randomly generate the labels and train the input layers (8 channels)
	\item Deep neural networks easily fit random data. It means that the effective capacity of neural networks is sufficient for memorizing the data.
\end{enumerate}

\subsubsection{Others}
\begin{enumerate}
	\item We test Efficientnet-B7 on ImageNet. The training accuracy is $95.87\%$ and the validation accuracy is $84.31\%$.	
\end{enumerate}
