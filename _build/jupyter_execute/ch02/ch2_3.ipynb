{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6e93d8c",
   "metadata": {},
   "source": [
    "# KL divergence and cross-entropy\n",
    "\n",
    "Cross-entropy minimization is frequently used in optimization and\n",
    "rare-event probability estimation. When comparing a distribution against\n",
    "a fixed reference distribution, cross-entropy and KL divergence are\n",
    "identical up to an additive constant. See more details in\n",
    "[@murphy2012machine; @kullback1951information; @kullback1997information]\n",
    "and the reference therein.\n",
    "\n",
    "The KL(Kullback--Leibler) divergence defines a special distance between\n",
    "two discrete probability distributions $$p=\\left( \\begin{array}{ccc}\n",
    "p_1\\\\\n",
    "\\vdots \\\\\n",
    "p_k\n",
    "\\end{array} \\right),\\quad  q=\\left( \\begin{array}{ccc}\n",
    "q_1\\\\\n",
    "\\vdots \\\\\n",
    "q_k\n",
    "\\end{array} \\right)$$ with $0\\le p_i, q_i\\le1$ and\n",
    "$\\sum_{i=1}^{k}p_i=\\sum_{i=1}^{k}q_i=1$ by $$\\label{KL-divergence}\n",
    "D_{\\rm KL}(q,p)= \\sum_{i=1}^k q_i\\log \\frac{q_i}{p_i}.$$\n",
    "\n",
    "::: lemma\n",
    "$D_{\\rm KL}(q,p)$ works like a \"distance\\\" without the symmetry:\n",
    "\n",
    "1.  $D_{\\rm KL}(q,p)\\ge0$;\n",
    "\n",
    "2.  $D_{\\rm KL}(q,p)=0$ if and only if $p=q$;\n",
    ":::\n",
    "\n",
    "::: proof\n",
    "*Proof.* We first note that the elementary inequality\n",
    "$$\\log x \\le x - 1, \\quad\\mathrm{for\\ any\\ }x\\ge0,$$ and the equality\n",
    "holds if and only if $x=1$.\n",
    "$$-D_{\\rm KL}(q,p) = - \\sum_{i=1}^c q_i\\log \\frac{q_i}{p_i}   = \\sum_{i=1}^k q_i\\log \\frac{p_i}{q_i} \\le \\sum_{i=1}^k q_i( \\frac{p_i}{q_i}  - 1) = 0.$$\n",
    "And the equality holds if and only if\n",
    "$$\\frac{p_i}{q_i} = 1 \\quad \\forall i = 1:k.$$ ◻\n",
    ":::\n",
    "\n",
    "Define cross-entropy for distribution $p$ and $q$ by\n",
    "$$\\label{Cross-Entropy}\n",
    "H(q,p) = - \\sum_{i=1}^k q_i \\log p_i,$$ and the entropy for distribution\n",
    "$q$ by $$\\label{Entropy}\n",
    "H(q) = - \\sum_{i=1}^k q_i \\log q_i.$$ Note that\n",
    "$$D_{\\rm KL}(q,p)= \\sum_{i=1}^k q_i\\log \\frac{q_i}{p_i} =  \\sum_{i=1}^k q_i \\log q_i - \\sum_{i=1}^k q_i \\log p_i$$\n",
    "Thus, $$\\label{KLandEntropy}\n",
    "H(q,p) = H(q) + D_{\\rm KL}(q,p).$$ It follows from the relation\n",
    "[\\[KLandEntropy\\]](#KLandEntropy){reference-type=\"eqref\"\n",
    "reference=\"KLandEntropy\"} that $$\\label{EntropyandKL}\n",
    "\\mathop{\\arg\\min}_p D_{\\rm KL}(q,p)=\\mathop{\\arg\\min}_p H(q,p).$$\n",
    "\n",
    "The concept of cross-entropy can be used to define a loss function in\n",
    "machine learning and optimization. Let us assume $y_i$ is the true label\n",
    "for $x_i$, for example $y_i = e_{k_i}$ if $x_i \\in A_{k_i}$. Consider\n",
    "the predicted distribution $$\\label{key}\n",
    "\\bm p(x; \\bm \\theta) = \\frac{1}{\\sum\\limits_{i=1}^k e^{w_i x+b_i}}\n",
    "\\begin{pmatrix}\n",
    "e^{w_1 x+b_1}\\\\\n",
    "e^{w_2 x+b_2}\\\\\n",
    "\\vdots\\\\\n",
    "e^{w_k x+b_k}\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "p_1(x; \\bm\\theta) \\\\\n",
    "p_2(x; \\bm\\theta) \\\\\n",
    "\\vdots \\\\\n",
    "p_k(x; \\bm\\theta)\n",
    "\\end{pmatrix}$$ for any data $x \\in A$. By\n",
    "[\\[EntropyandKL\\]](#EntropyandKL){reference-type=\"eqref\"\n",
    "reference=\"EntropyandKL\"}, the minimization of KL divergence is\n",
    "equivalent to the minimization of the cross-entropy, namely\n",
    "$$\\mathop{\\arg\\min}_{\\theta} \\sum_{i=1}^N D_{\\rm KL}(y_i,\\bm p(x_i; \\bm \\theta)) = \\mathop{\\arg\\min}_{\\theta} \\sum_{i=1}^N H(y_i, \\bm p(x_i; \\bm \\theta)).$$\n",
    "Recall that we have all data\n",
    "$D = \\{(x_1,y_1),(x_2,y_2),\\cdots, (x_N, y_N)\\}$. Then, it is natural to\n",
    "consider the loss function as following:\n",
    "$$\\sum_{j=1}^N H(y_i, \\bm p(x_i; \\bm \\theta)),$$ which measures the\n",
    "distance between the real label and predicted one for all data. In the\n",
    "meantime, we can check that $$\\begin{aligned}\n",
    "\\sum_{j=1}^N H(y_j, \\bm p(x_j; \\bm \\theta))&=-\\sum_{j=1}^N y_j  \\cdot \\log  \\bm p(x_j; \\bm \\theta )\\\\\n",
    "&=-\\sum_{j=1}^N  \\log p_{i_j}(x_i; \\bm \\theta) \\quad (\\text{because}~y_j = e_{i_j}~\\text{for}~x_j \\in A_{i_j})\\\\\n",
    "&=-\\sum_{i=1}^k \\sum_{x\\in A_i}  \\log p_{i}(x; \\bm \\theta) \\\\\n",
    "&=-\\log \\prod_{i=1}^k \\prod_{x\\in A_i}   p_{i}(x; \\bm \\theta)\\\\\n",
    "& = L(\\theta)\n",
    "\\end{aligned}$$ with $L(\\theta)$ defined in\n",
    "[\\[logistic\\]](#logistic){reference-type=\"eqref\" reference=\"logistic\"}\n",
    "as\n",
    "$$L(\\bm \\theta) = - \\sum_{i=1}^k \\sum_{x\\in A_i} \\log p_{i}(x;\\bm \\theta).$$\n",
    "\n",
    "That is to say, the logistic regression loss function defined by\n",
    "likelihood in [\\[logistic\\]](#logistic){reference-type=\"eqref\"\n",
    "reference=\"logistic\"} is exact the loss function defined by measuring\n",
    "the distance between real label and predicted one via cross-entropy. We\n",
    "can note $$\\label{key}\n",
    "\\min_{\\bm \\theta} L_\\lambda(\\bm \\theta) \\Leftrightarrow \\min_{\\bm \\theta} \\sum_{j=1}^N H(y_i, \\bm p(x_i; \\bm \\theta)) + \\lambda R(\\|\\bm \\theta\\|) \n",
    "\\Leftrightarrow \\min_{\\bm \\theta} \\sum_{j=1}^N D_{\\rm KL}(y_i, \\bm p(x_i; \\bm \\theta)) + \\lambda R(\\|\\bm \\theta\\|).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c031c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}