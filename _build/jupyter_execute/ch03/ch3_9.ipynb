{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b6e9378",
   "metadata": {},
   "source": [
    "# 3.9 Bayesian Approach to Machine Learning\n",
    "\n",
    "## Goal\n",
    "\n",
    "-   Goal: Estimate an unknown distribution on X from data\n",
    "    $\\left\\{x_{j}\\right\\}_{j=1}^{n}$\n",
    "\n",
    "    -   Build a model\n",
    "\n",
    "        -   Set of parameters $\\Theta$\n",
    "\n",
    "        -   Family of distribution on X,\n",
    "            $$p(x|\\theta)\\quad \\mbox{with}\\quad \\theta \\in \\Theta$$\n",
    "\n",
    "        -   Prior distribution on the parameters $\\Theta$,\n",
    "            $$q(\\theta).$$\n",
    "\n",
    "    -   Use Bayes' Law\n",
    "        $$p(\\theta|x) p(x)=p(x \\  and \\   \\theta )=p(x | \\theta) p(\\theta)$$\n",
    "\n",
    "    -   Recall if $A_{1} , A_{2}$ are events:\n",
    "        $$p\\left(A_{1} | A_{2}\\right)=\\frac{P\\left(A_{1} \\cap A_{2}\\right)}{P\\left(A_{2}\\right)}$$\n",
    "        $$p(\\theta | x)=\\frac{p(x | \\theta) q(\\theta)}{p(x)}$$\n",
    "        $$p(\\theta |x) \\sim p(x | \\theta) q(\\theta)$$ where\n",
    "        $p(\\theta |x)$ is the posteriori distribution, $q(\\theta)$ is\n",
    "        the prior distribution and $p(x | \\theta)$ is the likelihood\n",
    "        function.\n",
    "\n",
    "-   Stant with: prior $q(\\theta)$\n",
    "\n",
    "-   Add data $x$, replace $q$ with the posterior\n",
    "    $$p(\\theta | x) \\sim p(x | \\theta) q(\\theta)$$\n",
    "\n",
    "-   More data: multiply by likelihood function and then normalize\n",
    "\n",
    "-   Left with a posterior distribution\n",
    "\n",
    "    -   Sample from posterion distribution to approximate\n",
    "        $$P_{pred}(x)=\\int_{\\Theta} p(x | \\theta) p(\\theta) d \\theta$$\n",
    "\n",
    "    -   Choose $\\theta$ to maximize posterior distribution\n",
    "        $$\\begin{aligned}\n",
    "            \\theta^{*}&=\\arg \\max _{\\theta \\in \\Theta} p(\\theta|x)\\\\\n",
    "         &=\\arg \\min _{\\theta \\in \\Theta}-\\log p(\\theta|x)\\\\\n",
    "        &=\\arg \\min _{\\theta \\in \\Theta}-\\log (p(x | \\theta))-\\log (q(\\theta))+\\log (p(x))\\\\\n",
    "        &=\\arg \\min _{\\theta \\in \\Theta}-\\log (p(x | \\theta))-\\log (q(\\theta))\\end{aligned}$$\n",
    "        where $-\\log (p(x | \\theta))$ is the negative log likelihood and\n",
    "        $-\\log (q(\\theta))$ is the regularization coming from prior.\n",
    "\n",
    "    ## Example: Image Classification/ Logistic Regression\n",
    "\n",
    "-   Images $x \\in X=\\mathbb{R}^{d}$\n",
    "\n",
    "-   Labels $y \\in Y=\\left\\{e_{1}, \\ldots, e_{k}\\right\\}$ with $k$ is the\n",
    "    dimension of labels\n",
    "\n",
    "-   Data $\\left\\{(x_{j},y_{j})\\right\\}_{j=1}^{n}$\n",
    "\n",
    "-   Model\n",
    "    $\\theta=(W, b) \\in \\mathbb{R}^{k \\times d} \\times \\mathbb{R}^{k}$.\n",
    "    By [\\[eq:pp\\]](#eq:pp){reference-type=\"eqref\" reference=\"eq:pp\"},\n",
    "    $$p(y | x, \\theta)=\\frac{e^{Wx+b} \\cdot y}{e^{Wx+b} \\cdot \\mathbb{I}}$$\n",
    "\n",
    "-   Prior distribution: suppose it is a Gaussian,\n",
    "    $$q(W, b)=C e^{-\\alpha\\left(\\left\\|W\\right\\|_{2}^{2}+ \\left\\|b\\right\\|_{2}^{2}\\right)}$$\n",
    "\n",
    "    -   Calculate the posterior: here $q(W,b)=p(W, b|x)$,\n",
    "        $$p(W, b | x, y)=\\frac{p(y | x, W, b) p(W, b|x) }{p(y | x)}$$\n",
    "        $$p(y | x)=\\int_{\\Theta} p(y|x, W, b) q(W, b) d \\theta$$\n",
    "        $$\\begin{aligned}\n",
    "            (W, b)^{*}&=\\underset{W,b}{\\arg \\min }-\\log \\left( p\\left(W, b |\\left\\{x_{j},y_{j}\\right\\}_{j=1}^{n}\\right)\\right) \\\\\n",
    "        &=\\underset {W,b}\\arg  \\min -\\log \\left( p\\left(\\left\\lbrace y_{j}\\right\\rbrace _{j=1}^{n} | \\left\\lbrace x_{j}\\right\\rbrace _{j=1}^{n}, W, b\\right)\\right)-\\log (q(W, b))\\\\\n",
    "        &=\\underset {W,b}\\arg  \\min -\\log \\left(\\prod_{i=1}^{n} p\\left(y_{j} | x_{j}, W, b\\right)\\right)-\\log (q(W, b))\\\\\n",
    "        &=\\underset{W, b}{\\operatorname{argmin}} \\sum_{j=1}^{n} \\log \\left(e^{Wx+b} \\cdot \\mathbb{I}\\right)-\\log \\left(e^{Wx_j+b} \\cdot y_{j}\\right)+\\alpha\\left(\\|W\\|_{2}^{2}+\\| b\\|_{2}^{2}\\right).\n",
    "            \\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba32d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}