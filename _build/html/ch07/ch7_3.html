
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>7.3.4 Fourier transform of polynomials &#8212; Math 452 Site</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]]}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="7.2 Why we need deep neural networks via composition {#whydeep}" href="ch7_2.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/PSU_SCI_RGB_2C.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Math 452 Site</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to Math 452
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  contents
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch00/ch0_.html">
   Ch0 Get started: course information and preparations:
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch00/ch0_1.html">
     0.1 Course information, requirements and reference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch00/ch0_2.html">
     0.2 Course background and introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch00/ch0_3.html">
     0.3 Introduction to Python and Pytorch
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch01/ch1_.html">
   Ch1 Machine Learning and Image Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch01/video.html">
     Chapter 1 video
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch01/ch1_1_video.html">
     1.1 A basic machine learning problem: image classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch01/ch1_2_video.html">
     1.2 Image classification problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch01/ch1_3_video.html">
     1.3 Some popular data sets in image classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch01/hw2.html">
     Homework 1
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch02/ch2_.html">
   Ch2 Linear Machine Learning Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch02/ch2_1_video.html">
     2.1 Definition of linearly separable sets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch02/ch2_2.html">
     2.2 Introduction to logistic regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch02/ch2_3.html">
     2.3 KL divergence and cross-entropy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch02/ch2_4.html">
     2.4 Support vector machine
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch03/ch3_.html">
   Ch3 Probability
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_1.html">
     3.1 Introduction to probability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_2.html">
     3.2 Basic probability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_3.html">
     3.3 Basic Probability Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_4.html">
     3.4 Random, Variable, Mean, Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_5.html">
     3.5 Probability interpretation of logistic regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_6.html">
     3.6 Maximamum Likelihood
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_7.html">
     3.7 Basic Statistical Learning Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_8.html">
     3.8 Classfication/ Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_9.html">
     3.9 Bayesian Approach to Machine Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch03/ch3_10.html">
     3.10 General Covariance
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch04/ch4_.html">
   Chapter 4 Training Algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch04/ch4_1.html">
     4.1 Line search and gradient descent method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch04/ch4_2.html">
     4.2 Stochastic gradient descent method and convergence theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch05/ch5_.html">
   Chapter 5 Polynomials and Weierstrass theorem
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch05/ch5_1.html">
     5.1 Weierstrass Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch05/ch5_2.html">
     5.2 Fourier transform and Fourier series
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="ch7_.html">
   Chapter 7 Deep Neural Network Functions
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="ch7_1.html">
     7.1 Motivation: from finite element to neural network {#FE2NN}
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ch7_2.html">
     7.2 Why we need deep neural networks via composition {#whydeep}
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     7.3.4 Fourier transform of polynomials
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/ch07/ch7_3.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/liuzhengqi1996/math452/main?urlpath=lab/tree/ch07/ch7_3.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># 7.3 Definition of deep neural networks (DNN)

In this section, we will give a brief introduction to a special function
class related to deep neural networks (DNN) used in machine learning. We
then explore the relationship between DNN (with ReLU as activation
function) and linear finite element methods.

Given $n, m\ge 1$, the first ingredient in defining a deep neural
network (DNN) is (vector) linear functions of the form
$$\label{thetamap1}
\theta:\mathbb{R}^{n}\to\mathbb{R}^{m} ,$$as $\theta(x)=Wx+b$ where
$W=(w_{ij})\in\mathbb{R}^{m\times n}$, $b\in\mathbb{R}^{m}$. The second
main ingredient is a nonlinear activation function, usually denoted as
$$\label{sigma}
\sigma: \mathbb{R} \to \mathbb{R}.$$ By applying the function to each
component, we can extend this naturally to
$$\sigma:\mathbb R^{n}\mapsto \mathbb R^{n}.$$

## 7.3.1 Definition of neurons

1.  Primary variables $n_0=d$ $$x^0=x=
        \begin{pmatrix}
        x_1\\
        x_2\\
        \vdots \\  
        x_{d}
        \end{pmatrix}$$

2.  $n_1$ hyperplanes $\theta^{0}(x^0) = W^0 x + b^0$ where
    $W^0: \mathbb{R}^{d} \mapsto \mathbb{R}^{n_1}$: $$W^0x+b^0=
        \begin{pmatrix}
        w^0_1x+b^0_1\\
        w^0_2x+b^0_2\\
        \vdots \\  
        w^0_{n_1}x+b^0_{n_1}
        \end{pmatrix}\quad \mbox{with }\quad W^0=
        \begin{pmatrix}
        w^0_1\\
        w^0_2\\
        \vdots \\  
        w^0_{n_1}
        \end{pmatrix},\quad b^0=
        \begin{pmatrix}
        b^0_1\\
        b^0_2\\
        \vdots \\  
        b^0_{n_1}
        \end{pmatrix}$$

3.  $n_1$-neurons: $$x^1=\sigma(W^0x+b^0)
        =\begin{pmatrix}
        \sigma(w^0_1x+b^0_1)\\
        \sigma(w^0_2x+b^0_2)\\
        \vdots \\  
        \sigma(w^0_{n_1}x+b^0_{n_1})
        \end{pmatrix}$$

4.  $n_2$-hyperplanes $\theta^{1}(x^1) = W^1 x + b^1$ where
    $W^1: \mathbb{R}^{n_1} \mapsto \mathbb{R}^{n_2}$: $$W^1x^1+b^1=
        \begin{pmatrix}
        w^1_1x^1+b^1_1\\
        w^1_2x^1+b^1_2\\
        \vdots \\  
        w^1_{n_2}x^1+b^1_{n_2}
        \end{pmatrix}\quad \mbox{with }\quad 
        W^1=
        \begin{pmatrix}
        w^1_1 \\
        w^1_2 \\
        \vdots \\  
        w^1_{n_2} 
        \end{pmatrix},\ 
        b^1=
        \begin{pmatrix}
        b^1_1\\
        b^1_2\\
        \vdots \\  
        b^1_{n_2}
        \end{pmatrix}$$

5.  $n_2$-neurons: $$x^2=\sigma(W^1x+b^1)
        =\begin{pmatrix}
        \sigma(w^1_1x+b^1_1)\\
        \sigma(w^1_2x+b^1_2)\\
        \vdots \\  
        \sigma(w^1_{n_2}x+b^1_{n_2})
        \end{pmatrix}$$

6.  $\cdots$

## 7.3.2 Definition of deep neural network functions {#sec:DNN}

Given $d, k\in\mathbb{N}^+$ and
$$n_1,\dots,n_{k}\in\mathbb{N} \mbox{ with }n_0=d, n_{k+1}=1,$$ a
general DNN function from $\mathbb{R}^d$ to $\mathbb{R}$ is given by
$$\begin{aligned}
f^0(x)   &amp;=\theta^0(x) \\ 
f^{\ell}(x) &amp;= [  \theta^{\ell} \circ \sigma ](f^{\ell-1}(x)) \quad \ell = 1:k \\
f(x) &amp;= f^k(x). \end{aligned}$$ The following more concise notation is
often used in computer science literature: $$\label{compress-dnn}
f(x) = \theta^{k}\circ \sigma \circ \theta^{k-1} \circ \sigma \cdots \circ \theta^1 \circ \sigma \circ \theta^0(x),$$
here $\theta^i: \mathbb{R}^{n_{i}}\to\mathbb{R}^{n_{i+1}}$ are linear
functions as defined in
[\[thetamap1\]](#thetamap1){reference-type=&quot;eqref&quot;
reference=&quot;thetamap1&quot;}. Such a DNN is called a $(k+1)$-layer DNN, and is
said to have $k$-hidden layers. The size of this DNN is
$n_1+\cdots+n_k$.

Thus, we have the following connection of neurons and DNN functions
$$f^k(x) = \theta^{k}(x^k) = \theta^{k} \circ \sigma \circ \theta^{k-1}(x^{k-1}) = [\theta^{k} \circ \sigma ] (f^{k-1}),$$
or we can see that
$$x^k = \sigma(f^{k-1}) = \sigma \circ \theta^{k-1} \circ \sigma (f^{k-2}) = [\sigma \circ \theta^{k-1}] (x^{k-1}).$$
Based on these notation and connections, we have the following
definition of general artificial neural network functions.

Shallow (one hidden layer) neural network functions: $$\label{NN1}
\dnn(\sigma; n_1) 
=\bigg\{ f^1(x) = \theta^1 (x^1), \mbox{ with } W^\ell\in \mathbb R^{n_{\ell+1}\times
    n_{\ell}}, b^\ell\in\mathbb R^{n_\ell}, \ell=0, 1, n_0=d, n_2 = 1\bigg\}$$
Deep neural network functions: $$\label{NNL}
\dnn(\sigma; n_1,n_2,\ldots, n_L)=\bigg\{ f^{L}(x) = \theta^L (x^{L}), 
 \mbox{ with } W^\ell\in \mathbb R^{n_{\ell+1}\times
    n_{\ell}}, b^\ell\in\mathbb R^{n_\ell}, \ell=0:L, n_0=d, n_{L+1}=1\bigg\}$$
If we ignore the width (number of neurons) of network functions, we may
denote the general deep neural network functions with certain layers.

The 1-hidden layer (shallow) neural network is defined as:
$$\dnn=\dnn(\sigma) = \dnn^1(\sigma)
=\bigcup_{n_1\ge 1} \dnn(\sigma;n_1,1)$$ Generally, we can define the
L-hidden layer neural network as:
$$\dnn^L(\sigma) := \bigcup_{n_1, n_2, \cdots, n_{L}\ge 1} \dnn(\sigma;n_1,n_2,\cdots,n_L, 1).$$

## 7.3.3 ReLU DNN

In this section, we mainly consider a special activation function, known
as the *rectified linear unit* (ReLU), and defined as $\rm
ReLU: \mathbb R\mapsto \mathbb R$, $$\label{relu}
 {\rm ReLU}(x):=\max(0,x), \quad x\in\mathbb{R}.$$ A ReLU DNN with $k$
hidden layers might be written as: $$\label{relu-dnn}
f(x) = \theta^{k}\circ {\rm ReLU} \circ \theta^{k-1} \circ {\rm ReLU} \cdots \circ \theta^1 \circ {\rm ReLU} \circ \theta^0(x).$$

We note that $\rm ReLU$ is a continuous piecewise linear (CPWL)
function. Since the composition of two CPWL functions is still a CPWL
function, we have the following observation [@arora2016understanding].

::: {.lemma}
[\[dnn-cpwl\]]{#dnn-cpwl label=&quot;dnn-cpwl&quot;} Every ReLU DNN:
$\mathbb{R}^d\to\mathbb{R}^c$ is a continuous piecewise linear function.
More specifically, given any ReLU DNN, there is a polyhedral
decomposition of $\mathbb R^d$ such that this ReLU DNN is linear on each
polyhedron in such a decomposition.
:::

Here is a simple example for the &quot;grid\&quot; created by some 2-layer ReLU
DNNs in $\mathbb{R}^2$.

![Projections of the domain partitions formed by 2-layer ReLU DNNs with
sizes
$(n_0, n_1, n_2, n_3)= (2, 5, 5, 1), (2, 10, 10, 1) \text{and}\ (2, 20, 20, 1)$
with random
parameters.](figures/2to5to5to1-eps-converted-to.pdf &quot;fig:&quot;){#fig:dnn-region
width=&quot;.3\\textwidth&quot;} ![Projections of the domain partitions formed by
2-layer ReLU DNNs with sizes
$(n_0, n_1, n_2, n_3)= (2, 5, 5, 1), (2, 10, 10, 1) \text{and}\ (2, 20, 20, 1)$
with random
parameters.](figures/2to10to10to1-eps-converted-to.pdf &quot;fig:&quot;){#fig:dnn-region
width=&quot;.3\\textwidth&quot;} ![Projections of the domain partitions formed by
2-layer ReLU DNNs with sizes
$(n_0, n_1, n_2, n_3)= (2, 5, 5, 1), (2, 10, 10, 1) \text{and}\ (2, 20, 20, 1)$
with random
parameters.](figures/2to20to20to1-eps-converted-to.pdf &quot;fig:&quot;){#fig:dnn-region
width=&quot;.3\\textwidth&quot;}

For convenience of exposition, we introduce the following notation:
Namely $\dnn^L({\sigma})$ represents the DNN model with $L$ hidden
layers and ReLU activation function with arbitrary size, if
$\sigma = {\rm ReLU}$.
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">  File</span><span class="nn"> &quot;&lt;ipython-input-1-c9b0ab945119&gt;&quot;</span><span class="gt">, line </span><span class="mi">3</span>
    <span class="n">In</span> <span class="n">this</span> <span class="n">section</span><span class="p">,</span> <span class="n">we</span> <span class="n">will</span> <span class="n">give</span> <span class="n">a</span> <span class="n">brief</span> <span class="n">introduction</span> <span class="n">to</span> <span class="n">a</span> <span class="n">special</span> <span class="n">function</span>
       <span class="o">^</span>
<span class="ne">SyntaxError</span>: invalid syntax
</pre></div>
</div>
</div>
</div>
<div class="section" id="fourier-transform-of-polynomials">
<h1>7.3.4 Fourier transform of polynomials<a class="headerlink" href="#fourier-transform-of-polynomials" title="Permalink to this headline">¶</a></h1>
<p>We begin by noting that an activation function <span class="math notranslate nohighlight">\(\sigma\)</span>, which satisfies
a polynomial growth condition <span class="math notranslate nohighlight">\(|\sigma(x)| \leq C(1 + |x|)^n\)</span> for some
constants <span class="math notranslate nohighlight">\(C\)</span> and <span class="math notranslate nohighlight">\(n\)</span>, is a tempered distribution. As a result, we make
this assumption on our activation functions in the following theorems.
We briefly note that this condition is sufficient, but not necessary
(for instance an integrable function need not satisfy a pointwise
polynomial growth bound) for <span class="math notranslate nohighlight">\(\sigma\)</span> to be represent a tempered
distribution.</p>
<p>We begin by studying the convolution of <span class="math notranslate nohighlight">\(\sigma\)</span> with a Gaussian
mollifier. Let <span class="math notranslate nohighlight">\(\eta\)</span> be a Gaussian mollifier
$<span class="math notranslate nohighlight">\(\eta(x) = \frac{1}{\sqrt{\pi}}e^{-x^2}.\)</span><span class="math notranslate nohighlight">\( Set
\)</span>\eta_\epsilon=\frac{1}{\epsilon}\eta(\frac{x}{\epsilon})<span class="math notranslate nohighlight">\(. Then
consider \)</span><span class="math notranslate nohighlight">\(\label{sigma-epsilon}
\sigma_{\epsilon}(x):=\sigma\ast{\eta_\epsilon}(x)=\int_{\mathbb{R}}\sigma(x-y){\eta_\epsilon}(y)dy\)</span><span class="math notranslate nohighlight">\(
for a given activation function \)</span>\sigma<span class="math notranslate nohighlight">\(. It is clear that
\)</span>\sigma_{\epsilon}\in C^\infty(\mathbb{R})<span class="math notranslate nohighlight">\(. Moreover, by considering
the Fourier transform (as a tempered distribution) we see that
\)</span><span class="math notranslate nohighlight">\(\label{eq_278}
 \hat{\sigma}_{\epsilon} = \hat{\sigma}\hat{\eta}_{\epsilon} = \hat{\sigma}\eta_{\epsilon^{-1}}.\)</span>$</p>
<p>We begin by stating a lemma which characterizes the set of polynomials
in terms of their Fourier transform.</p>
<p>As an application of Lemma
<a class="reference external" href="#polynomial_lemma">[polynomial_lemma]</a>{reference-type=”ref”
reference=”polynomial_lemma”}, we give a simple proof of the result in
the next section.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./ch07"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="ch7_2.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">7.2 Why we need deep neural networks via composition {#whydeep}</p>
            </div>
        </a>
    </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Department of Mathematics, Penn State University Park<br/>
        
            &copy; Copyright The Pennsylvania State University, 2021. This material is not licensed for resale.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-206078372-1', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>