{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Introduction to logistic regression\n",
    "\n",
    "In this section, we introduce techniques related to basic logistic\n",
    "regression, see [@gelman2006data] for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"500\"\n",
       "            src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_3im0zbc7&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_5pdspks0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff6c2a9e910>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_3im0zbc7&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_5pdspks0\" ,width='800', height='500')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"500\"\n",
       "            src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_awemnq71&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_g1ldr79i\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff6c2a9e880>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://cdnapisec.kaltura.com/p/2356971/sp/235697100/embedIframeJs/uiconf_id/41416911/partner_id/2356971?iframeembed=true&playerId=kaltura_player&entry_id=1_awemnq71&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[leadWithHTML5]=true&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_g1ldr79i\" ,width='800', height='500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "Assume that we are given $k$ linearly separable sets\n",
    "$A_1,A_2,\\cdots,A_k\\in \\mathbb{R}^d$, we define the set of classifiable\n",
    "weights as\n",
    "$$\\bm\\Theta = \\{\\bm\\theta = (W,b): w_ix+b_i>w_jx+b_j,~\\forall x\\in A_i, j\\neq i, i= 1,\\cdots,k\\}$$\n",
    "which means those $(W,b)$ can separate $A_1,A_2,\\cdots,A_k$ correctly.\n",
    "\n",
    "Our linearly separable assumption implies that\n",
    "$\\bm\\Theta\\neq \\emptyset$. Now we know the existence of linearly\n",
    "classifiable weights. But how can we find one element in $\\bm\\Theta$?\n",
    "\n",
    "```{adnomition} Definition\n",
    "[\\[softmax\\]]{#softmax label=\"softmax\"} Given\n",
    "$s = (s_1,s_2,\\cdots,s_k)^T\\in \\mathbb{R}^k$, we define the soft-max\n",
    "mapping $\\sigma: \\mathbb{R}^k \\rightarrow\\mathbb{R}^k$ as\n",
    "\n",
    "$$\n",
    "    \\sigma(s)  = \\frac{e^{s}}{e^{s}\\cdot \\bm{1}} = \\frac{1}{\\sum\\limits_{i=1}^k e^{s_i}}\n",
    "    \\begin{pmatrix}\n",
    "    e^{s_1}\\\\\n",
    "    e^{s_2}\\\\\n",
    "    \\vdots\\\\\n",
    "    e^{s_k}\n",
    "    \\end{pmatrix}\n",
    "$$ \n",
    "\n",
    "where $e^{s} = \n",
    "\\begin{pmatrix}\n",
    "e^{s_1}\\\\\n",
    "e^{s_2}\\\\\n",
    "\\vdots\\\\\n",
    "e^{s_k}\n",
    "\\end{pmatrix}$, $\\bm{1} = \n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1\n",
    "\\end{pmatrix} \\in\\mathbb{R}^k$.\n",
    "```\n",
    "\n",
    "```{admonition} Definition\n",
    "Given parameter $\\bm\\theta = (W,b)$, we define a feature mapping\n",
    "$\\bm p: \\mathbb{R}^d \\rightarrow \\mathbb{R}^k$ as\n",
    "\n",
    "$$\n",
    "    \\bm p(x; \\bm\\theta)  = \\sigma(Wx+b) = \\frac{1}{\\sum\\limits_{i=1}^k e^{w_i x+b_i}}\n",
    "    \\begin{pmatrix}\n",
    "    e^{w_1 x+b_1}\\\\\n",
    "    e^{w_2 x+b_2}\\\\\n",
    "    \\vdots\\\\\n",
    "    e^{w_k x+b_k}\n",
    "    \\end{pmatrix}\n",
    "    = \\begin{pmatrix}\n",
    "    p_1(x; \\bm\\theta) \\\\\n",
    "    p_2(x; \\bm\\theta) \\\\\n",
    "    \\vdots \\\\\n",
    "    p_k(x; \\bm\\theta)\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "    \n",
    "where the $i$-th component \n",
    "    \n",
    "$$\n",
    "    \\label{key}\n",
    "    p_i(x; \\bm\\theta) = \\frac{e^{w_i x+b_i}}{\\sum\\limits_{i=1}^k e^{w_i x+b_i}}.\n",
    "$$\n",
    "```\n",
    "\n",
    "\n",
    "The soft-max mapping have several important properties.\n",
    "\n",
    "1.  $\\displaystyle 0< p_i(x; \\bm\\theta) <1,~\\sum_i p_i(x; \\bm\\theta) = 1$.\n",
    "\n",
    "    This implies that $\\bm p(x; \\bm\\theta)$ can be regarded as a\n",
    "    probability distribution of data points which means that given\n",
    "    $x\\in \\mathbb{R}^d$, we have $x\\in A_i$ with probability\n",
    "    $p_i(x; \\bm{\\theta})$, $i = 1,\\cdots,k$.\n",
    "\n",
    "2.  $p_i(x; \\bm\\theta)>p_j(x; \\bm\\theta)\\Leftrightarrow w_ix+b_i>w_j x+b_j.$\n",
    "\n",
    "    This implies that the linearly classifiable weights have an\n",
    "    equivalent description as\n",
    "$$\n",
    "    \\bm{\\Theta} = \\left\\{\\bm\\theta: p_i(x; \\bm\\theta)>p_j(x; \\bm\\theta),~\\forall x\\in A_i, j\\neq i, i= 1,\\cdots,k\\right\\}\n",
    "$$\n",
    "\n",
    "3.  We usually use the max-out method to do classification. For a given\n",
    "    data point $x$, we first use a soft-max mapping to map it to\n",
    "    $\\bm p(x; \\bm\\theta)$, then we attach $x$ to the class\n",
    "    $i= \\arg\\max_j p_i(x; \\bm\\theta)$.\n",
    "\n",
    "    This means that we pick the label $i$ as the class of $x$ such that\n",
    "    $x\\in A_i$ has the biggest probability $p_i(x; \\bm\\theta)$.\n",
    "\n",
    "More detailed discussion of logistic regression from the probability\n",
    "perspective will be presented in the nearly future.\n",
    "\n",
    "From the above properties, we can define the following likelihood\n",
    "function to help find elements in $\\bm{\\Theta}$: \n",
    "\n",
    "$$\n",
    "P (\\bm\\theta)=\n",
    "\\prod\\limits_{i = 1}^k \\prod\\limits_{x\\in A_i} p_i(x; \\bm\\theta).\n",
    "$$\n",
    "    \n",
    "Based on the property that \n",
    "\n",
    "$$\n",
    "p_i (x; \\bm \\theta) = \\max_{1\\le j \\le k} p_j(x; \\bm \\theta), \\quad\\forall x \\in A_i,\\ \\bm \\theta \\in \\Theta,\n",
    "$$ (key1)\n",
    "\n",
    "we may use the next optimization problem \n",
    "\n",
    "$$ \n",
    "\\max_{\\bm \\theta\\in \\bm{\\Theta}} P(\\bm \\theta).\n",
    "$$ (key2)\n",
    "    \n",
    "to find an element in\n",
    "$\\bm{\\Theta}$. More precisely, let us introduce the next lemmas\n",
    "(properties) of $P(\\bm \\theta)$.\n",
    "\n",
    "```{admonition} Lemma\n",
    "[\\[lemm:H1/2\\]]{#lemm:H1/2 label=\"lemm:H1/2\"} Assume that the sets\n",
    "$A_1,A_2,\\cdots,A_k$ are linearly separable. Then we have\n",
    "\n",
    "$$\n",
    "    \\left\\{\\bm \\theta:~P(\\bm\\theta)>\\frac{1}{2}\\right\\}\\subset \\bm{\\Theta}.\n",
    "$$\n",
    "```\n",
    "\n",
    "```{admonition} Proof\n",
    "*Proof.* It suffices to show that if $\\bm\\theta \\not\\in \\bm\\Theta$, we\n",
    "must have $P(\\bm\\theta)\\leq\\frac{1}{2}$. For any $\\bm\\theta \\not\\in\n",
    "    \\bm\\Theta$, there must exist an $i_0$ ,an $x_0\\in A_{i_0}$ and a\n",
    "$j_0\\neq i_0$ such that\n",
    "\n",
    "$$\n",
    "    w_{i_0} x_0 + b_{i_0} \\leq w_{j_0}x_0 + b_{j_0}.\n",
    "$$ \n",
    "\n",
    "Then we have\n",
    "\n",
    "$$\n",
    "    p_{i_0}(x_0; \\bm\\theta) \\leq \\frac{e^{w_{i_0} x_0 + b_{i_0}}}{e^{w_{i_0} x_0+b_{i_0}}+e^{w_{j_0} x_0+b_{j_0}}} \\leq\\frac{1}{2}.\n",
    "$$\n",
    "    \n",
    "Notice that $p_i(x; \\bm \\theta) < 1$ for all $i = 1,\\cdots,k$, $x\\in A$.\n",
    "So \n",
    "\n",
    "$$\n",
    "    P(\\bm\\theta) <  p_{i_0}(x_0; \\bm\\theta) \\leq \\frac{1}{2}.\n",
    "$$ \n",
    "```\n",
    "\n",
    "```{admonition} lemma\n",
    "If $A_1,A_2,\\cdots,A_k$ are linearly separable and\n",
    "$\\bm\\theta \\in \\bm\\Theta$, we have\n",
    "\n",
    "$$\n",
    "    \\lim_{\\alpha\\rightarrow +\\infty}p_i(x; \\alpha\\bm\\theta) = 1\\Leftrightarrow x\\in A_i.\n",
    "$$\n",
    "```\n",
    "\n",
    "```{admonition} proof\n",
    "*Proof.* We first note that if $x\\in A_i$,\n",
    "\n",
    "$$\n",
    "    p_i(x,\\bm \\theta) = \\frac{1}{1+\\sum\\limits_{j\\neq i}e^{\\alpha[(w_j x+ b_j)-(w_i x+b_i)]}} \\to 1, \\quad \\text{as} \\quad \\alpha \\to \\infty.\n",
    "$$\n",
    "    \n",
    "On the other hand, if $x\\not\\in A_i$,\n",
    "\n",
    "$$\n",
    "    p_i(x; \\bm\\alpha\\bm\\theta) = \\frac{1}{1+\\sum\\limits_{j\\neq i}e^{\\alpha[(w_j x+ b_j)-(w_i x+b_i)]}} \\leq \\frac{1}{2}.\n",
    "$$\n",
    "    \n",
    "This implies that if $x\\not\\in A_i$,\n",
    "$\\lim_{\\alpha\\rightarrow \\infty}p_i(x; \\alpha\\bm \\theta)\\neq 1$ which is\n",
    "equivalent to the proposition that if\n",
    "$\\lim_{\\alpha\\rightarrow \\infty}p_i(x; \\alpha\\bm \\theta)= 1$, then\n",
    "$x\\in A_i$. \n",
    "```\n",
    "\n",
    "```{admonition} Lemma\n",
    ":name: thm1\n",
    "If $A_1,A_2,\\cdots,A_k$ are linearly\n",
    "separable,\n",
    "\n",
    "$$\n",
    "    \\bm\\Theta = \\left\\{\\bm\\theta: \\lim_{\\alpha\\rightarrow +\\infty}P(\\alpha\\bm\\theta) = 1\\right\\}.\n",
    "$$\n",
    "```\n",
    "\n",
    "```{admonition} Proof\n",
    "*Proof.* We first note that if $\\bm\\theta \\in\\bm\\Theta$, we have {ref}`thm1`\n",
    "$\\displaystyle\\lim_{\\alpha\\rightarrow +\\infty}p_i(x; \\alpha\\bm\\theta) = 1$\n",
    "for all $x\\in A_i$. So\n",
    "\n",
    "$$\n",
    "    \\lim\\limits_{\\alpha\\rightarrow +\\infty} P(\\alpha\\bm\\theta) = \\lim\\limits_{\\alpha\\rightarrow +\\infty} \\prod\\limits_{i = 1}^k \\prod\\limits_{x\\in A_i} p_i(x; \\alpha\\bm\\theta) = \\prod\\limits_{i = 1}^k \\prod\\limits_{x\\in A_i} \\lim\\limits_{\\alpha\\rightarrow +\\infty}p_i(x; \\alpha\\bm\\theta) = 1.\n",
    "$$\n",
    "    \n",
    "On the other hand, if\n",
    "$\\lim\\limits_{\\alpha\\rightarrow +\\infty} P(\\alpha\\bm\\theta) = 1$, there\n",
    "must exist one $\\alpha_0>0$ such that\n",
    "$P(\\alpha_0\\bm\\theta) >\\frac{1}{2}$. From `{ref}, we have $\\alpha_0\\bm\\theta\\in\\bm\\Theta$, which\n",
    "means $\\bm\\theta\\in\\bm\\Theta$. \n",
    "```\n",
    "\n",
    "These properties above imply that if we can obtain a classifiable weight\n",
    "through maximizing $P(\\bm\\theta)$, while lemma\n",
    "[\\[thm2\\]](#thm2){reference-type=\"ref\" reference=\"thm2\"} tells us that\n",
    "$P(\\bm\\theta)$ will not have a global minimum actually.\n",
    "\n",
    "More specifically, we just need to find some $\\bm \\theta \\in \\Theta$\n",
    "such that $$\\label{key}\n",
    "P(\\bm \\theta) > \\frac{1}{2} \\Leftrightarrow  L(\\bm \\theta) : = -\\log P(\\bm \\theta )  < \\log(2).$$\n",
    "\n",
    "## Regularized logistic regression\n",
    "\n",
    "Here, we start from the regularization term\n",
    "$e^{-\\lambda R(\\|\\bm\\theta\\|)}$ with these next properties:\n",
    "\n",
    "1.  $\\lambda > 0$.\n",
    "\n",
    "2.  $R(t)$ is a strictly increasing function on $\\mathbb{R}^+$ with\n",
    "    $R(0) = 0$, $\\lim\\limits_{t\\rightarrow +\\infty} R(t) = +\\infty$. For\n",
    "    example, $R(t) = t^2$.\n",
    "\n",
    "3.  $\\|\\cdot\\|$ is a norm on $R^{k\\times(d+1)}$, a commonly used norm is\n",
    "    the following Frobenius norm: $$\\label{key}\n",
    "        \\|\\bm \\theta\\|_F = \\sqrt{\\sum_{i,j}W_{ij}^2 + \\sum_i b_i^2}.$$\n",
    "\n",
    "Based on this regularization term, we may consider the following\n",
    "regularized likelihood function $P_\\lambda(\\bm\\theta)$ as $$\\label{key}\n",
    " P_\\lambda(\\bm\\theta) = P(\\bm\\theta)e^{-\\lambda R(\\|\\bm\\theta\\|)}.$$\n",
    "\n",
    "Here, let us define $$\\label{key}\n",
    "\\bm\\Theta_{\\lambda} = \\mathop{{\\arg\\max}}_{\\bm\\theta}  P_\\lambda(\\bm\\theta),$$\n",
    "where $$\\label{key}\n",
    "\\mathop{\\arg\\max}_{\\bm\\theta}  P_\\lambda(\\bm\\theta) = \\left\\{\\bm \\theta ~:~ P_\\lambda(\\bm \\theta) = \\max_{\\bm \\theta} P_\\lambda(\\bm \\theta) \\right\\}.$$\n",
    "\n",
    "The next lemma show that the maximal set of modified objective is not\n",
    "empty.\n",
    "\n",
    "```{admonition} lemma\n",
    "Suppose that $A_1,A_2, \\cdots, A_k$ are linearly separable, then\n",
    "\n",
    "1.  if $\\lambda = 0$, $\\bm\\Theta_0 = \\emptyset$,\n",
    "\n",
    "2.  $\\bm\\Theta_{\\lambda}$ must be nonempty for all $\\lambda>0$.\n",
    "```\n",
    "\n",
    "```{admonition} proof\n",
    "*Proof.* Lemma [\\[thm2\\]](#thm2){reference-type=\"ref\" reference=\"thm2\"}\n",
    "shows the first proposition. For the second proposition, we notice that\n",
    "\n",
    "1.  $P_\\lambda(\\bm 0) = \\frac{1}{k^N}$.\n",
    "\n",
    "2.  $\\exists\\ M_{\\lambda}>0$ such that\n",
    "    $e^{-\\lambda R(\\|\\bm\\theta\\|)}<\\frac{1}{k^N}$ whenever\n",
    "    $\\|\\bm\\theta\\|> M_{\\lambda}$ because of the properties of\n",
    "    $R(\\|\\bm\\theta\\|)$.\n",
    "\n",
    "So a maxima on $\\{\\bm\\theta: \\|\\bm\\theta\\| \\le M_{\\lambda}\\}$ must be a\n",
    "global maxima. Then we can easily obtain the result in the lemma from\n",
    "the boundedness and closeness of\n",
    "$\\{\\bm\\theta: \\|\\bm\\theta\\| \\le M_{\\lambda}\\}$. ◻\n",
    "```\n",
    "\n",
    "Furthermore, we have the next theorem which shows that we can indeed get\n",
    "$\\Theta$ by maximizing $P_\\lambda(\\bm \\theta)$.\n",
    "\n",
    "```{admonition} theorem\n",
    "[\\[thm-L-Theta\\]]{#thm-L-Theta label=\"thm-L-Theta\"} If\n",
    "$A_1,A_2,\\cdots,A_k$ are linearly separable, $$\\label{key}\n",
    "    \\bm\\Theta_{\\lambda} \\subset \\bm\\Theta,$$ when $\\lambda>0$ and\n",
    "sufficiently small.\n",
    "```\n",
    "\n",
    "```{admonition} proof\n",
    "*Proof.* By Lemma [\\[lemm:H1/2\\]](#lemm:H1/2){reference-type=\"ref\"\n",
    "reference=\"lemm:H1/2\"}, we can take $\\bm\\theta_0\\in \\bm\\Theta$ such that\n",
    "$P(\\bm\\theta_0)> \\frac{3}{4}$. Then, for any\n",
    "$\\lambda < \\frac{\\log \\frac{3}{2}}{R(\\|\\bm\\theta_0\\|)}$,\n",
    "$\\bm\\theta_{\\lambda}\\in \\bm\\Theta_{\\lambda}$, we have\n",
    "$$P(\\bm\\theta_{\\lambda}) \\geq  P_\\lambda(\\bm\\theta_{\\lambda})  \\geq P_\\lambda(\\bm \\theta_0) = P(\\bm\\theta_0)e^{-\\lambda R(\\|\\bm\\theta_0\\|)} > \\frac{3}{4}\\cdot \\frac{2}{3} = \\frac{1}{2},$$\n",
    "which implies that $\\bm \\theta_{\\lambda} \\in \\Theta$. Thus, for any\n",
    "$0< \\lambda < \\frac{\\log \\frac{3}{2}}{R(\\|\\bm\\theta_0\\|)}$,\n",
    "$\\bm\\Theta_{\\lambda} \\subset \\bm\\Theta$. ◻\n",
    "```\n",
    "\n",
    "The design of logistic regression is that maximize\n",
    "$P_\\lambda(\\bm\\theta)$ is equivalent to minimize\n",
    "$-\\log P_\\lambda(\\bm\\theta)$, i.e., $$\\label{key}\n",
    "\\max_{\\bm \\theta} \\left\\{ P_\\lambda(\\bm\\theta) \\right\\} \\Leftrightarrow \\min_{\\bm \\theta} \\left\\{ -\\log   P_\\lambda(\\bm\\theta)\\right\\},$$\n",
    "while the second one is more convenient to evaluate the gradient.\n",
    "Meanwhile, we add a regularization term $R(\\bm\\theta)$ to the objective\n",
    "function which makes the optimization problem has a unique solution.\n",
    "\n",
    "Mathematically, we can formulate Logistic regression as\n",
    "$$\\min_{\\bm\\theta} L_\\lambda(\\bm \\theta),$$ where\n",
    "$$\\label{eq:logisticlambda}\n",
    "L_\\lambda(\\bm \\theta)  := -\\log P_\\lambda(\\bm\\theta) = -\\log P(\\bm\\theta) + \\lambda R(\\|\\bm\\theta\\|) = L(\\bm\\theta) + \\lambda R(\\|\\bm\\theta\\|),$$\n",
    "with $$\\label{logistic}\n",
    "L(\\bm \\theta) = - \\sum_{i=1}^k \\sum_{x\\in A_i} \\log p_{i}(x;\\bm \\theta).$$\n",
    "\n",
    "Then we have the next logistic regression algorithm.\n",
    "\n",
    "```{admonition} algorithm\n",
    "Given data $A_1, A_2, \\cdots, A_k$, find $$\\label{key}\n",
    "    \\bm \\theta^* = \\mathop{\\arg\\min}_{\\bm \\theta}  L_\\lambda(\\bm \\theta),$$\n",
    "for some sufficient small $\\lambda > 0$.\n",
    "```\n",
    "\n",
    "```{admonition} remark\n",
    "Here $$\\label{key}\n",
    "    L(\\bm \\theta)  = -\\log P(\\bm\\theta),$$ is known as the loss function\n",
    "of logistic regression model. The next reasons may show that why\n",
    "$L(\\bm \\theta)$ is popular.\n",
    "\n",
    "1.  It is more convenient to take gradient for $L(\\bm \\theta)$ than\n",
    "    $P(\\bm \\theta)$.\n",
    "\n",
    "2.  $L(\\bm \\theta)$ is related the so-called cross-entropy loss function\n",
    "    which will be discussed in the next section.\n",
    "\n",
    "3.  $L(\\bm \\theta)$ is a convex function which will be discussed later.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
